{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-objectives",
   "metadata": {},
   "source": [
    "# 01 â€” Exploratory Data Analysis: Customer Lifetime Value (CLV)\n",
    "\n",
    "**Dataset:** UCI Online Retail II  \n",
    "**Source:** UK-based online gift retailer, transactions 01 Dec 2009 -- 09 Dec 2011  \n",
    "**Objective:** Predict 12-month customer lifetime value (CLV) using a temporal split strategy.\n",
    "\n",
    "---\n",
    "\n",
    "## Temporal Split Strategy\n",
    "\n",
    "| Window | Period | Purpose |\n",
    "|--------|--------|---------|\n",
    "| **Observation** | Dec 2009 -- Nov 2010 (12 months) | Compute RFM features and behavioral signals |\n",
    "| **Prediction** | Dec 2010 -- Dec 2011 (12 months) | Compute target: `clv_12m = sum(Quantity * Price)` per customer |\n",
    "\n",
    "## CLV Definition\n",
    "\n",
    "For each customer active in the observation window:\n",
    "- **clv_12m** = total revenue attributed to that customer in the prediction window\n",
    "- Customers with zero purchases in the prediction window have `clv_12m = 0` (churned)\n",
    "- **Cold-start customers** (fewer than 2 purchases in observation) receive a median CLV prediction at inference time\n",
    "\n",
    "## Sections\n",
    "\n",
    "1. Introduction & Objectives  \n",
    "2. Load & Inspect Raw Data  \n",
    "3. Data Quality Assessment  \n",
    "4. Temporal Coverage  \n",
    "5. Customer Purchase Patterns  \n",
    "6. CLV Label Analysis  \n",
    "7. RFM Segmentation Preview  \n",
    "8. Feature Correlations  \n",
    "9. Key Findings Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Paths\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "SHARED_RAW = PROJECT_ROOT.parent / 'shared' / 'data' / 'raw'\n",
    "PROCESSED_DIR = PROJECT_ROOT / 'data' / 'processed'\n",
    "CSV_PATH = SHARED_RAW / 'online_retail_ii.csv'\n",
    "\n",
    "# Temporal split boundaries\n",
    "OBS_START = pd.Timestamp('2009-12-01')\n",
    "OBS_END = pd.Timestamp('2010-11-30 23:59:59')\n",
    "PRED_START = pd.Timestamp('2010-12-01')\n",
    "PRED_END = pd.Timestamp('2011-12-31 23:59:59')\n",
    "\n",
    "print(f'Project root:  {PROJECT_ROOT}')\n",
    "print(f'Raw CSV path:  {CSV_PATH}')\n",
    "print(f'File exists:   {CSV_PATH.exists()}')\n",
    "print(f'Processed dir: {PROCESSED_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section2-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Load & Inspect Raw Data\n",
    "\n",
    "**Intent:** Load the raw CSV, inspect its schema, shape, and basic statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-raw-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(CSV_PATH, parse_dates=['InvoiceDate'])\n",
    "\n",
    "print(f'Shape: {df.shape[0]:,} rows x {df.shape[1]} columns')\n",
    "print(f'Date range: {df[\"InvoiceDate\"].min()} -- {df[\"InvoiceDate\"].max()}')\n",
    "print(f'\\nColumn dtypes:')\n",
    "print(df.dtypes)\n",
    "print(f'\\n--- First 5 rows ---')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "describe-raw",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=== Numeric Summary ===')\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "null-pct",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=== Null Percentage Per Column ===')\n",
    "null_pct = df.isnull().mean().mul(100).round(2)\n",
    "null_pct_df = null_pct.to_frame('null_pct').assign(\n",
    "    null_count=df.isnull().sum()\n",
    ")[['null_count', 'null_pct']]\n",
    "print(null_pct_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section3-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Data Quality Assessment\n",
    "\n",
    "**Intent:** Identify data quality issues that the pipeline must handle: null customer IDs, cancellations, non-product stock codes, invalid prices/quantities, and duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "null-customer-id",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Null Customer ID ---\n",
    "null_cid = df['Customer ID'].isna().sum()\n",
    "null_cid_pct = df['Customer ID'].isna().mean() * 100\n",
    "print(f'Rows with null Customer ID: {null_cid:,} ({null_cid_pct:.1f}%)')\n",
    "print(f'Unique Customer IDs (non-null): {df[\"Customer ID\"].nunique():,}')\n",
    "print()\n",
    "\n",
    "# How much revenue is lost by dropping null-CID rows?\n",
    "df['Revenue'] = df['Quantity'] * df['Price']\n",
    "rev_total = df['Revenue'].sum()\n",
    "rev_null_cid = df.loc[df['Customer ID'].isna(), 'Revenue'].sum()\n",
    "print(f'Total revenue (all rows): {rev_total:,.0f}')\n",
    "print(f'Revenue in null-CID rows: {rev_null_cid:,.0f} ({rev_null_cid / rev_total * 100:.1f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cancellations",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cancellations (Invoice starts with 'C') ---\n",
    "is_cancel = df['Invoice'].astype(str).str.startswith('C')\n",
    "cancel_count = is_cancel.sum()\n",
    "cancel_pct = is_cancel.mean() * 100\n",
    "print(f'Cancellation rows: {cancel_count:,} ({cancel_pct:.2f}%)')\n",
    "print()\n",
    "\n",
    "# Revenue impact of cancellations\n",
    "cancel_rev = df.loc[is_cancel, 'Revenue'].sum()\n",
    "print(f'Revenue in cancellation rows: {cancel_rev:,.0f}')\n",
    "print('(Negative revenue = returned value)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "non-product-stockcodes",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Non-product StockCodes ---\n",
    "# Common non-product codes: POST, DOT, M, BANK CHARGES, PADS, etc.\n",
    "non_product_mask = (\n",
    "    df['StockCode'].astype(str).str.match(r'^[A-Za-z]')\n",
    "    & ~df['StockCode'].astype(str).str.match(r'^C\\d')  # exclude normal codes starting with C + digits\n",
    ")\n",
    "non_product_codes = df.loc[non_product_mask, 'StockCode'].value_counts().head(15)\n",
    "print(f'Rows with non-product StockCodes: {non_product_mask.sum():,} ({non_product_mask.mean()*100:.2f}%)')\n",
    "print(f'\\nTop non-product StockCodes:')\n",
    "print(non_product_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invalid-price-qty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Price <= 0 or Quantity <= 0 ---\n",
    "bad_price = (df['Price'] <= 0).sum()\n",
    "bad_qty = (df['Quantity'] <= 0).sum()\n",
    "print(f'Rows with Price <= 0:    {bad_price:,} ({bad_price / len(df) * 100:.2f}%)')\n",
    "print(f'Rows with Quantity <= 0: {bad_qty:,} ({bad_qty / len(df) * 100:.2f}%)')\n",
    "print()\n",
    "\n",
    "# Overlap between negative quantity and cancellations\n",
    "neg_qty_and_cancel = ((df['Quantity'] <= 0) & is_cancel).sum()\n",
    "neg_qty_only = ((df['Quantity'] <= 0) & ~is_cancel).sum()\n",
    "print(f'Negative Qty AND cancellation: {neg_qty_and_cancel:,}')\n",
    "print(f'Negative Qty but NOT cancellation: {neg_qty_only:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "duplicates",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Duplicates ---\n",
    "n_dups = df.duplicated().sum()\n",
    "print(f'Exact duplicate rows: {n_dups:,} ({n_dups / len(df) * 100:.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quality-summary-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Quality Summary Table ---\n",
    "quality_summary = pd.DataFrame({\n",
    "    'Issue': [\n",
    "        'Null Customer ID',\n",
    "        'Cancellations (Invoice starts with C)',\n",
    "        'Non-product StockCodes',\n",
    "        'Price <= 0',\n",
    "        'Quantity <= 0',\n",
    "        'Exact duplicates'\n",
    "    ],\n",
    "    'Row Count': [\n",
    "        null_cid,\n",
    "        cancel_count,\n",
    "        non_product_mask.sum(),\n",
    "        bad_price,\n",
    "        bad_qty,\n",
    "        n_dups\n",
    "    ],\n",
    "    'Pct of Total': [\n",
    "        f'{null_cid_pct:.1f}%',\n",
    "        f'{cancel_pct:.2f}%',\n",
    "        f'{non_product_mask.mean()*100:.2f}%',\n",
    "        f'{bad_price / len(df) * 100:.2f}%',\n",
    "        f'{bad_qty / len(df) * 100:.2f}%',\n",
    "        f'{n_dups / len(df) * 100:.2f}%'\n",
    "    ]\n",
    "})\n",
    "print('=== Data Quality Summary ===')\n",
    "print(quality_summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section4-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Temporal Coverage\n",
    "\n",
    "**Intent:** Visualize transaction volume over time and verify the observation/prediction window boundaries. Understand customer cohort arrival patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monthly-volume",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monthly transaction volume\n",
    "df['YearMonth'] = df['InvoiceDate'].dt.to_period('M')\n",
    "monthly_counts = df.groupby('YearMonth').size()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "x_dates = monthly_counts.index.to_timestamp()\n",
    "ax.bar(x_dates, monthly_counts.values, width=25, color='steelblue', edgecolor='white', alpha=0.85)\n",
    "\n",
    "# Mark observation and prediction windows\n",
    "ax.axvline(OBS_START, color='green', linestyle='--', linewidth=2, label='Observation Start (Dec 2009)')\n",
    "ax.axvline(OBS_END, color='orange', linestyle='--', linewidth=2, label='Observation End (Nov 2010)')\n",
    "ax.axvline(PRED_START, color='red', linestyle='--', linewidth=2, label='Prediction Start (Dec 2010)')\n",
    "ax.axvline(PRED_END, color='darkred', linestyle='--', linewidth=2, label='Prediction End (Dec 2011)')\n",
    "\n",
    "ax.set_title('Monthly Transaction Volume with Temporal Split Boundaries', fontsize=14)\n",
    "ax.set_xlabel('Month')\n",
    "ax.set_ylabel('Transaction Count')\n",
    "ax.legend(loc='upper left', fontsize=9)\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Monthly transaction counts:')\n",
    "print(monthly_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monthly-revenue",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monthly revenue (positive transactions only)\n",
    "df_pos = df[(df['Quantity'] > 0) & (df['Price'] > 0) & (~is_cancel)].copy()\n",
    "df_pos['Revenue'] = df_pos['Quantity'] * df_pos['Price']\n",
    "monthly_revenue = df_pos.groupby(df_pos['InvoiceDate'].dt.to_period('M'))['Revenue'].sum()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "x_dates_rev = monthly_revenue.index.to_timestamp()\n",
    "ax.bar(x_dates_rev, monthly_revenue.values, width=25, color='mediumseagreen', edgecolor='white', alpha=0.85)\n",
    "\n",
    "ax.axvline(OBS_END, color='orange', linestyle='--', linewidth=2, label='Split boundary')\n",
    "\n",
    "ax.set_title('Monthly Revenue (Positive Transactions Only)', fontsize=14)\n",
    "ax.set_xlabel('Month')\n",
    "ax.set_ylabel('Revenue')\n",
    "ax.legend()\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cohort-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer cohort analysis: month of first appearance\n",
    "df_with_cust = df.dropna(subset=['Customer ID']).copy()\n",
    "first_purchase = df_with_cust.groupby('Customer ID')['InvoiceDate'].min()\n",
    "cohort_month = first_purchase.dt.to_period('M')\n",
    "cohort_counts = cohort_month.value_counts().sort_index()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "x_cohort = cohort_counts.index.to_timestamp()\n",
    "ax.bar(x_cohort, cohort_counts.values, width=25, color='coral', edgecolor='white', alpha=0.85)\n",
    "\n",
    "ax.axvline(OBS_END, color='orange', linestyle='--', linewidth=2, label='Split boundary')\n",
    "\n",
    "ax.set_title('New Customer Cohorts by Month (First Purchase Date)', fontsize=14)\n",
    "ax.set_xlabel('Month')\n",
    "ax.set_ylabel('New Customers')\n",
    "ax.legend()\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Observation-period vs prediction-period new customers\n",
    "obs_custs = first_purchase[first_purchase <= OBS_END].count()\n",
    "pred_only_custs = first_purchase[first_purchase > OBS_END].count()\n",
    "print(f'Customers first seen in observation window: {obs_custs:,}')\n",
    "print(f'Customers first seen in prediction window only: {pred_only_custs:,}')\n",
    "print(f'(Prediction-only customers will NOT be in training data)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section5-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Customer Purchase Patterns\n",
    "\n",
    "**Intent:** Understand the distributions of frequency, monetary value, and recency in the observation window. These form the basis of RFM features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "obs-window-stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to observation window, valid transactions only\n",
    "obs_mask = (\n",
    "    (df_with_cust['InvoiceDate'] >= OBS_START)\n",
    "    & (df_with_cust['InvoiceDate'] <= OBS_END)\n",
    "    & (df_with_cust['Quantity'] > 0)\n",
    "    & (df_with_cust['Price'] > 0)\n",
    "    & (~df_with_cust['Invoice'].astype(str).str.startswith('C'))\n",
    ")\n",
    "obs_df = df_with_cust[obs_mask].copy()\n",
    "obs_df['Revenue'] = obs_df['Quantity'] * obs_df['Price']\n",
    "\n",
    "print(f'Observation-window transactions: {len(obs_df):,}')\n",
    "print(f'Unique customers in observation window: {obs_df[\"Customer ID\"].nunique():,}')\n",
    "\n",
    "# Compute per-customer RFM-like stats\n",
    "ref_date = OBS_END + pd.Timedelta(days=1)\n",
    "customer_stats = obs_df.groupby('Customer ID').agg(\n",
    "    frequency=('Invoice', 'nunique'),\n",
    "    monetary=('Revenue', 'sum'),\n",
    "    recency_days=('InvoiceDate', lambda x: (ref_date - x.max()).days),\n",
    "    avg_order_value=('Revenue', 'mean'),\n",
    "    n_items=('Quantity', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "print(f'\\n=== Customer-Level Summary (Observation Window) ===')\n",
    "print(customer_stats[['frequency', 'monetary', 'recency_days']].describe().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "purchase-patterns-plots",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# (a) Frequency distribution\n",
    "ax = axes[0, 0]\n",
    "freq_clipped = customer_stats['frequency'].clip(upper=customer_stats['frequency'].quantile(0.99))\n",
    "ax.hist(freq_clipped, bins=50, color='steelblue', edgecolor='white', alpha=0.85)\n",
    "ax.set_title('Purchase Frequency (Observation Window)', fontsize=12)\n",
    "ax.set_xlabel('Number of Unique Invoices')\n",
    "ax.set_ylabel('Customers')\n",
    "median_freq = customer_stats['frequency'].median()\n",
    "ax.axvline(median_freq, color='red', linestyle='--', label=f'Median = {median_freq:.0f}')\n",
    "ax.legend()\n",
    "\n",
    "# (b) Monetary distribution (log scale)\n",
    "ax = axes[0, 1]\n",
    "monetary_pos = customer_stats.loc[customer_stats['monetary'] > 0, 'monetary']\n",
    "ax.hist(np.log10(monetary_pos), bins=50, color='mediumseagreen', edgecolor='white', alpha=0.85)\n",
    "ax.set_title('Monetary Value -- log10 (Observation Window)', fontsize=12)\n",
    "ax.set_xlabel('log10(Total Revenue)')\n",
    "ax.set_ylabel('Customers')\n",
    "median_mon = monetary_pos.median()\n",
    "ax.axvline(np.log10(median_mon), color='red', linestyle='--',\n",
    "           label=f'Median = {median_mon:,.0f}')\n",
    "ax.legend()\n",
    "\n",
    "# (c) Recency distribution\n",
    "ax = axes[1, 0]\n",
    "ax.hist(customer_stats['recency_days'], bins=50, color='coral', edgecolor='white', alpha=0.85)\n",
    "ax.set_title('Recency in Days (Observation Window)', fontsize=12)\n",
    "ax.set_xlabel('Days Since Last Purchase')\n",
    "ax.set_ylabel('Customers')\n",
    "median_rec = customer_stats['recency_days'].median()\n",
    "ax.axvline(median_rec, color='red', linestyle='--', label=f'Median = {median_rec:.0f} days')\n",
    "ax.legend()\n",
    "\n",
    "# (d) Frequency vs Monetary scatter\n",
    "ax = axes[1, 1]\n",
    "scatter_data = customer_stats[customer_stats['monetary'] > 0]\n",
    "ax.scatter(scatter_data['frequency'], scatter_data['monetary'],\n",
    "           alpha=0.3, s=10, color='mediumpurple')\n",
    "ax.set_title('Frequency vs Monetary Value', fontsize=12)\n",
    "ax.set_xlabel('Purchase Frequency')\n",
    "ax.set_ylabel('Total Revenue')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xscale('log')\n",
    "\n",
    "plt.suptitle('Customer Purchase Patterns -- Observation Window', fontsize=14, y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section6-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. CLV Label Analysis\n",
    "\n",
    "**Intent:** Analyze the target variable (`clv_12m`) produced by the data pipeline. Understand its distribution, zero-inflation, and relationship to observation-period behavior.\n",
    "\n",
    "> **Note:** This section requires `data/processed/clv_labels.csv` from the data pipeline. If the pipeline has not run yet, this section will show instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-clv-labels",
   "metadata": {},
   "outputs": [],
   "source": [
    "clv_labels_path = PROCESSED_DIR / 'clv_labels.csv'\n",
    "\n",
    "if clv_labels_path.exists():\n",
    "    clv_labels = pd.read_csv(clv_labels_path)\n",
    "    print(f'CLV labels loaded: {len(clv_labels):,} customers')\n",
    "    print(f'Columns: {clv_labels.columns.tolist()}')\n",
    "    print()\n",
    "    print(clv_labels.describe().round(2))\n",
    "else:\n",
    "    clv_labels = None\n",
    "    print('clv_labels.csv not found. Run the data pipeline first:')\n",
    "    print('  cd m02_clv')\n",
    "    print('  /Users/aayan/MarketingAnalytics/.venv/bin/python src/data_pipeline.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clv-distribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "if clv_labels is not None:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # (a) Raw CLV distribution\n",
    "    ax = axes[0]\n",
    "    clv_pos = clv_labels.loc[clv_labels['clv_12m'] > 0, 'clv_12m']\n",
    "    ax.hist(clv_pos, bins=100, color='steelblue', edgecolor='white', alpha=0.85)\n",
    "    ax.set_title('CLV Distribution (CLV > 0 only)', fontsize=12)\n",
    "    ax.set_xlabel('CLV (12-month revenue)')\n",
    "    ax.set_ylabel('Customers')\n",
    "    ax.set_xlim(0, clv_pos.quantile(0.99))\n",
    "    median_clv = clv_pos.median()\n",
    "    ax.axvline(median_clv, color='red', linestyle='--',\n",
    "               label=f'Median = {median_clv:,.0f}')\n",
    "    ax.legend()\n",
    "\n",
    "    # (b) Log-scale CLV distribution\n",
    "    ax = axes[1]\n",
    "    ax.hist(np.log1p(clv_labels['clv_12m']), bins=100, color='mediumseagreen',\n",
    "            edgecolor='white', alpha=0.85)\n",
    "    ax.set_title('CLV Distribution -- log1p scale (all customers)', fontsize=12)\n",
    "    ax.set_xlabel('log1p(CLV)')\n",
    "    ax.set_ylabel('Customers')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Summary stats\n",
    "    n_zero = (clv_labels['clv_12m'] == 0).sum()\n",
    "    n_total = len(clv_labels)\n",
    "    pct_zero = n_zero / n_total * 100\n",
    "    print(f'Customers with CLV = 0 (churned): {n_zero:,} / {n_total:,} ({pct_zero:.1f}%)')\n",
    "    print(f'Customers with CLV > 0 (retained): {n_total - n_zero:,} ({100 - pct_zero:.1f}%)')\n",
    "    print(f'Mean CLV (all):  {clv_labels[\"clv_12m\"].mean():,.2f}')\n",
    "    print(f'Median CLV (all): {clv_labels[\"clv_12m\"].median():,.2f}')\n",
    "    print(f'Mean CLV (CLV>0): {clv_pos.mean():,.2f}')\n",
    "    print(f'Max CLV: {clv_labels[\"clv_12m\"].max():,.2f}')\n",
    "else:\n",
    "    print('Skipping -- clv_labels.csv not available.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clv-decile",
   "metadata": {},
   "outputs": [],
   "source": [
    "if clv_labels is not None:\n",
    "    # CLV by decile\n",
    "    clv_sorted = clv_labels['clv_12m'].sort_values(ascending=False).reset_index(drop=True)\n",
    "    clv_labels_sorted = clv_labels.sort_values('clv_12m', ascending=False).copy()\n",
    "    clv_labels_sorted['decile'] = pd.qcut(\n",
    "        clv_labels_sorted['clv_12m'].rank(method='first'),\n",
    "        q=10, labels=[f'D{i}' for i in range(1, 11)]\n",
    "    )\n",
    "\n",
    "    decile_stats = clv_labels_sorted.groupby('decile', observed=True)['clv_12m'].agg(\n",
    "        ['mean', 'median', 'sum', 'count']\n",
    "    ).round(2)\n",
    "    decile_stats['pct_of_total_revenue'] = (decile_stats['sum'] / decile_stats['sum'].sum() * 100).round(1)\n",
    "    print('=== CLV by Decile ===')\n",
    "    print(decile_stats)\n",
    "\n",
    "    # Pareto visualization\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    ax.bar(decile_stats.index.astype(str), decile_stats['pct_of_total_revenue'],\n",
    "           color='steelblue', edgecolor='white', alpha=0.85)\n",
    "    ax.set_title('Revenue Concentration by Customer Decile', fontsize=14)\n",
    "    ax.set_xlabel('Customer Decile (D10 = highest CLV)')\n",
    "    ax.set_ylabel('% of Total Revenue')\n",
    "    for i, v in enumerate(decile_stats['pct_of_total_revenue']):\n",
    "        ax.text(i, v + 0.5, f'{v:.1f}%', ha='center', fontsize=9)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print('Skipping -- clv_labels.csv not available.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clv-by-obs-purchases",
   "metadata": {},
   "outputs": [],
   "source": [
    "if clv_labels is not None:\n",
    "    # Box plot of CLV by number of observation-period purchases\n",
    "    plot_data = clv_labels.copy()\n",
    "\n",
    "    # Cap n_obs_purchases for cleaner visualization\n",
    "    cap_val = int(plot_data['n_obs_purchases'].quantile(0.95))\n",
    "    cap_label = f'{cap_val}+'\n",
    "    plot_data['obs_purchases_binned'] = plot_data['n_obs_purchases'].clip(upper=cap_val)\n",
    "    plot_data['obs_purchases_label'] = plot_data['obs_purchases_binned'].astype(str)\n",
    "    plot_data.loc[plot_data['n_obs_purchases'] >= cap_val, 'obs_purchases_label'] = cap_label\n",
    "\n",
    "    # Use log1p for better visibility\n",
    "    plot_data['log_clv'] = np.log1p(plot_data['clv_12m'])\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    sorted_labels = sorted(plot_data['obs_purchases_binned'].unique())\n",
    "    label_strs = [str(int(x)) if x < cap_val else cap_label for x in sorted_labels]\n",
    "\n",
    "    box_data = [plot_data.loc[plot_data['obs_purchases_binned'] == v, 'log_clv'].values\n",
    "                for v in sorted_labels]\n",
    "    bp = ax.boxplot(box_data, labels=label_strs, patch_artist=True,\n",
    "                    boxprops=dict(facecolor='steelblue', alpha=0.6),\n",
    "                    medianprops=dict(color='red', linewidth=2))\n",
    "\n",
    "    ax.set_title('log1p(CLV) by Number of Observation-Period Purchases', fontsize=14)\n",
    "    ax.set_xlabel('Number of Purchases in Observation Window')\n",
    "    ax.set_ylabel('log1p(CLV 12m)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Cold-start analysis\n",
    "    if 'is_cold_start' in clv_labels.columns:\n",
    "        n_cold = clv_labels['is_cold_start'].sum()\n",
    "        pct_cold = n_cold / len(clv_labels) * 100\n",
    "        print(f'Cold-start customers (< 2 obs purchases): {n_cold:,} ({pct_cold:.1f}%)')\n",
    "        print(f'Mean CLV for cold-start: {clv_labels.loc[clv_labels[\"is_cold_start\"] == 1, \"clv_12m\"].mean():,.2f}')\n",
    "        print(f'Mean CLV for non-cold-start: {clv_labels.loc[clv_labels[\"is_cold_start\"] == 0, \"clv_12m\"].mean():,.2f}')\n",
    "else:\n",
    "    print('Skipping -- clv_labels.csv not available.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section7-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. RFM Segmentation Preview\n",
    "\n",
    "**Intent:** Load the engineered customer features and inspect RFM score distributions. Visualize how recency and frequency relate to mean CLV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-features",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_path = PROCESSED_DIR / 'customer_features.csv'\n",
    "\n",
    "if features_path.exists():\n",
    "    features_df = pd.read_csv(features_path)\n",
    "    print(f'Customer features loaded: {features_df.shape}')\n",
    "    print(f'Columns: {features_df.columns.tolist()}')\n",
    "    print()\n",
    "    print(features_df.head())\n",
    "else:\n",
    "    features_df = None\n",
    "    print('customer_features.csv not found. Run the data pipeline + feature engineering first:')\n",
    "    print('  cd m02_clv')\n",
    "    print('  /Users/aayan/MarketingAnalytics/.venv/bin/python src/data_pipeline.py')\n",
    "    print('  /Users/aayan/MarketingAnalytics/.venv/bin/python src/feature_engineering.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rfm-distributions",
   "metadata": {},
   "outputs": [],
   "source": [
    "if features_df is not None:\n",
    "    # Look for RFM score columns\n",
    "    rfm_cols = [c for c in features_df.columns if 'score' in c.lower() or 'rfm' in c.lower()]\n",
    "    print(f'RFM-related columns found: {rfm_cols}')\n",
    "\n",
    "    if len(rfm_cols) >= 2:\n",
    "        n_plots = len(rfm_cols)\n",
    "        fig, axes = plt.subplots(1, n_plots, figsize=(5 * n_plots, 4))\n",
    "        if n_plots == 1:\n",
    "            axes = [axes]\n",
    "        for ax, col in zip(axes, rfm_cols):\n",
    "            vals = features_df[col].dropna()\n",
    "            ax.hist(vals, bins=30, color='steelblue', edgecolor='white', alpha=0.85)\n",
    "            ax.set_title(col, fontsize=12)\n",
    "            ax.set_xlabel('Score')\n",
    "            ax.set_ylabel('Customers')\n",
    "        plt.suptitle('RFM Score Distributions', fontsize=14, y=1.02)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print('No RFM score columns found; showing top feature distributions instead.')\n",
    "        numeric_cols = features_df.select_dtypes(include=[np.number]).columns[:6]\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "        for ax, col in zip(axes.flat, numeric_cols):\n",
    "            vals = features_df[col].dropna()\n",
    "            ax.hist(vals, bins=40, color='steelblue', edgecolor='white', alpha=0.85)\n",
    "            ax.set_title(col, fontsize=11)\n",
    "        plt.suptitle('Feature Distributions', fontsize=14, y=1.01)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print('Skipping -- customer_features.csv not available.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rfm-heatmap",
   "metadata": {},
   "outputs": [],
   "source": [
    "if features_df is not None and clv_labels is not None:\n",
    "    # Merge features with CLV labels\n",
    "    # Identify customer ID column (may be 'customer_id' or 'Customer ID')\n",
    "    cid_col_feat = [c for c in features_df.columns if 'customer' in c.lower() and 'id' in c.lower()]\n",
    "    cid_col_label = [c for c in clv_labels.columns if 'customer' in c.lower() and 'id' in c.lower()]\n",
    "\n",
    "    if cid_col_feat and cid_col_label:\n",
    "        merged = features_df.merge(clv_labels[[cid_col_label[0], 'clv_12m']],\n",
    "                                   left_on=cid_col_feat[0], right_on=cid_col_label[0],\n",
    "                                   how='inner')\n",
    "        print(f'Merged dataset: {len(merged):,} customers')\n",
    "\n",
    "        # Look for recency_score and frequency_score\n",
    "        rec_score = [c for c in merged.columns if 'recency' in c.lower() and 'score' in c.lower()]\n",
    "        freq_score = [c for c in merged.columns if 'frequency' in c.lower() and 'score' in c.lower()]\n",
    "\n",
    "        if rec_score and freq_score:\n",
    "            heatmap_data = merged.groupby([rec_score[0], freq_score[0]])['clv_12m'].mean().unstack()\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(8, 6))\n",
    "            sns.heatmap(heatmap_data, annot=True, fmt='.0f', cmap='YlOrRd', ax=ax)\n",
    "            ax.set_title('Mean CLV by Recency Score vs Frequency Score', fontsize=14)\n",
    "            ax.set_xlabel('Frequency Score')\n",
    "            ax.set_ylabel('Recency Score')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print('Recency/Frequency score columns not found. Skipping heatmap.')\n",
    "            print(f'Available columns: {merged.columns.tolist()}')\n",
    "    else:\n",
    "        print('Could not identify customer ID columns for merge.')\n",
    "else:\n",
    "    print('Skipping -- features or labels not available.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section8-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Feature Correlations\n",
    "\n",
    "**Intent:** Examine correlations between engineered features and the CLV target to guide feature selection and identify potential multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correlation-matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "if features_df is not None and clv_labels is not None:\n",
    "    # Build analysis dataframe\n",
    "    cid_col_feat = [c for c in features_df.columns if 'customer' in c.lower() and 'id' in c.lower()]\n",
    "    cid_col_label = [c for c in clv_labels.columns if 'customer' in c.lower() and 'id' in c.lower()]\n",
    "\n",
    "    if cid_col_feat and cid_col_label:\n",
    "        analysis_df = features_df.merge(\n",
    "            clv_labels[[cid_col_label[0], 'clv_12m']],\n",
    "            left_on=cid_col_feat[0], right_on=cid_col_label[0], how='inner'\n",
    "        )\n",
    "\n",
    "        # Select only numeric columns\n",
    "        numeric_df = analysis_df.select_dtypes(include=[np.number])\n",
    "        # Drop ID columns\n",
    "        id_cols = [c for c in numeric_df.columns if 'id' in c.lower()]\n",
    "        numeric_df = numeric_df.drop(columns=id_cols, errors='ignore')\n",
    "\n",
    "        print(f'Numeric features for correlation: {numeric_df.shape[1]} columns')\n",
    "\n",
    "        # Full correlation matrix\n",
    "        corr = numeric_df.corr()\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(16, 12))\n",
    "        mask = np.triu(np.ones_like(corr, dtype=bool), k=1)\n",
    "        sns.heatmap(corr, mask=mask, annot=True, fmt='.2f', cmap='RdBu_r',\n",
    "                    center=0, vmin=-1, vmax=1, ax=ax,\n",
    "                    annot_kws={'size': 8}, square=True)\n",
    "        ax.set_title('Feature Correlation Matrix (lower triangle)', fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print('Could not identify customer ID columns for merge.')\n",
    "else:\n",
    "    print('Skipping -- features or labels not available.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "top-correlations",
   "metadata": {},
   "outputs": [],
   "source": [
    "if features_df is not None and clv_labels is not None:\n",
    "    if 'clv_12m' in numeric_df.columns:\n",
    "        clv_corr = numeric_df.corr()['clv_12m'].drop('clv_12m').sort_values(ascending=False)\n",
    "\n",
    "        print('=== Top 10 Features Correlated with clv_12m ===')\n",
    "        print(clv_corr.head(10).round(4))\n",
    "        print()\n",
    "        print('=== Bottom 5 (Negative Correlations) ===')\n",
    "        print(clv_corr.tail(5).round(4))\n",
    "\n",
    "        # Bar chart of correlations\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        top_n = min(15, len(clv_corr))\n",
    "        top_features = clv_corr.head(top_n)\n",
    "        colors = ['steelblue' if v > 0 else 'coral' for v in top_features.values]\n",
    "        ax.barh(range(top_n), top_features.values, color=colors, edgecolor='white')\n",
    "        ax.set_yticks(range(top_n))\n",
    "        ax.set_yticklabels(top_features.index, fontsize=10)\n",
    "        ax.set_xlabel('Pearson Correlation with clv_12m')\n",
    "        ax.set_title('Top Features Correlated with CLV', fontsize=14)\n",
    "        ax.invert_yaxis()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print('clv_12m not found in numeric columns.')\n",
    "else:\n",
    "    print('Skipping -- features or labels not available.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section9-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Key Findings Summary\n",
    "\n",
    "### Dataset Overview\n",
    "- ~1M transaction rows spanning Dec 2009 -- Dec 2011 from a UK-based online retailer\n",
    "- ~5,900 unique customers with valid Customer IDs\n",
    "- 22.8% of rows lack Customer ID (must be dropped for customer-level modeling)\n",
    "- 91.9% of transactions originate from the United Kingdom\n",
    "\n",
    "### Data Quality\n",
    "- **Cancellations** (~1.8% of rows) are identified by Invoice prefix 'C' and carry negative quantities\n",
    "- **Non-product StockCodes** (e.g., POST, DOT, M) should be excluded from revenue calculations\n",
    "- **Duplicates** (~3.2%) exist and should be removed\n",
    "- **Zero/negative prices** are minimal but need filtering\n",
    "\n",
    "### Temporal Split\n",
    "- Observation window (Dec 2009 -- Nov 2010) provides sufficient data for RFM feature computation\n",
    "- Prediction window (Dec 2010 -- Dec 2011) captures the next 12 months for CLV target\n",
    "- New customers appearing only in the prediction window are excluded from training\n",
    "\n",
    "### CLV Target Characteristics\n",
    "- CLV is highly right-skewed -- a log1p transformation is recommended for the regression target\n",
    "- Significant fraction of customers have CLV = 0 (churned in prediction window)\n",
    "- Strong Pareto effect: top decile likely accounts for majority of total revenue\n",
    "- CLV increases monotonically with observation-period purchase frequency\n",
    "\n",
    "### Modeling Implications\n",
    "- **Target transform:** Use `log1p(clv_12m)` with `expm1()` inverse for predictions\n",
    "- **Cold-start handling:** Customers with < 2 observation-period purchases need special treatment (median CLV fallback)\n",
    "- **Feature candidates:** Frequency, monetary, and recency are strong CLV predictors (as expected from RFM theory)\n",
    "- **Baseline model:** BG/NBD + Gamma-Gamma probabilistic model from `lifetimes` library\n",
    "- **Primary model:** LightGBM Regressor on engineered features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 5,
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}