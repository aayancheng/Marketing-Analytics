# Tutorial Plan: Claude Code → End-to-End Analytics Project (1 Hour)

## Context
This tutorial introduces a team of first-time Claude Code users to agentic workflows by:
1. Showing a finished analytics model (m03_churn) as the "north star" showcase
2. Live-building m02_clv using the `/analytics-project` skill so they see the full pipeline in motion

The goal is conviction: Claude Code can research, build, and document a production-grade ML model end-to-end, including notebooks, FastAPI backend, React frontend, and slide deck — with a single skill invocation.

Audience: no prior CLI experience, unfamiliar with agent concepts or slash-command skills.

---

## Schedule (6 × 10-minute blocks)

---

### Block 1 — 0:00–0:10 | "What Is Claude Code?"

**Goal:** Get everyone oriented at the terminal with Claude Code running.

**Steps:**
1. Show installation: `npm install -g @anthropic/claude-code`
2. First launch: `claude` in a terminal — explain the chat interface, the `>` prompt
3. Type a simple question: "What files are in this folder?" → demo it uses real tools (Glob/Bash)
4. Key concept: "This is not a chatbot. It reads your files, runs commands, and edits code."
5. Point out: `claude --help`, the settings gear, permission prompts

**Key talking points:**
- Different from ChatGPT: Claude Code has tools (file read/write, bash, browser)
- It operates in your repo; all changes are local and reviewable
- Permission model: it asks before running destructive commands

---

### Block 2 — 0:10–0:20 | "Slash Commands, Skills, and Agentic Loops"

**Goal:** Explain how `/skills` extend Claude Code, and what an "agentic loop" means visually.

**Steps:**
1. Type `/help` — show built-in slash commands
2. Explain: skills are user-defined prompts stored in `~/.claude/skills/*.md`
3. Show `~/.claude/skills/analytics-project.md` briefly (scroll through phases)
4. Draw or whiteboard the loop: `Prompt → Agent spawns sub-agents → Tools run → Results feed back → Next phase`
5. Show `project_state.json` from m03_churn as evidence of checkpointing

**Key talking points:**
- Skills = reusable mega-prompts that encode best practices
- Sub-agents run in parallel (research, data, model phases use multiple agents)
- State file means you can resume if interrupted — not a one-shot prompt

**Files to reference:**
- [m03_churn/project_state.json](m03_churn/project_state.json) — shows completed phase history

---

### Block 3 — 0:20–0:30 | "The Finished Product: Tour m03_churn"

**Goal:** Walk through every artifact the skill produced — make the team feel the scope.

**Steps — file tour in editor (open VSCode/IDE side by side):**

| Artifact | Path | What to show |
|----------|------|--------------|
| Research brief | [m03_churn/docs/research_brief.md](m03_churn/docs/research_brief.md) | Domain context, KPIs, dataset selection rationale |
| EDA notebook | [m03_churn/notebooks/01_eda_churn.ipynb](m03_churn/notebooks/01_eda_churn.ipynb) | Distribution plots, churn rate breakdown |
| Model training | [m03_churn/notebooks/02_model_training.ipynb](m03_churn/notebooks/02_model_training.ipynb) | Calibration curve, AUC-ROC |
| SHAP diagnostics | [m03_churn/notebooks/03_model_diagnostics.ipynb](m03_churn/notebooks/03_model_diagnostics.ipynb) | Beeswarm plot, failure mode analysis |
| Model card | [m03_churn/docs/model_card.md](m03_churn/docs/model_card.md) | 2-page summary: purpose, users, limitations |
| Executive deck | [m03_churn/docs/executive_deck.md](m03_churn/docs/executive_deck.md) | 15-slide Marp deck for stakeholders |
| Feature pipeline | [m03_churn/src/feature_engineering.py](m03_churn/src/feature_engineering.py) | `FEATURE_COLUMNS` shared with API |
| FastAPI backend | [m03_churn/src/api/main.py](m03_churn/src/api/main.py) | Lifespan loader, `/api/customers`, `/api/predict` |
| React frontend | [m03_churn/app/client/src/App.jsx](m03_churn/app/client/src/App.jsx) | 3 views: Lookup, What-If, Segments |

**Key talking point:** "Every one of these files was generated by the skill — no human wrote them."

---

### Block 4 — 0:30–0:40 | "Live Demo: m03 App Running"

**Goal:** Run the three services, open the browser, demo all three views interactively.

**Steps — run in 3 terminals:**

```bash
# Terminal 1 — FastAPI
cd m03_churn
/Users/aayan/MarketingAnalytics/.venv/bin/python -m uvicorn src.api.main:app --port 8003 --reload

# Terminal 2 — Express proxy
cd m03_churn/app/server && npm start

# Terminal 3 — Vite frontend
cd m03_churn/app/client && npm run dev
```

Open `http://localhost:5175` and walk through each view:

1. **Lookup view** — Search for a customer, show the churn gauge + SHAP risk factors
2. **What-If view** — Drag a slider (e.g. contract length), watch the score update live
3. **Segment view** — Show KPI cards, filter high-risk tier, browse the table

**Talking points:**
- "The backend scored all 7,043 customers at startup — no per-request ML inference"
- "SHAP explains *why* a customer is high-risk — not just a score"
- "This is a deployable internal tool, not a prototype"

---

### Block 5 — 0:40–0:50 | "Live Build: Kick Off m02_clv"

**Goal:** Run the analytics-project skill, narrate what's happening phase by phase.

**Pre-check (do this before the tutorial):**
- Confirm UCI data at [shared/data/raw/](shared/data/raw/) (no download needed)
- Confirm `.venv` packages: `lifetimes`, `lightgbm`, `shap`, `pyarrow` installed

**Live command:**
```
/analytics-project "Customer Lifetime Value (CLV) prediction for e-commerce" --project-dir m02_clv
```

**Narrate each phase as it starts:**

| Phase | What to point at | What to say |
|-------|-----------------|-------------|
| Research (Phase 1) | 3 parallel sub-agents spawning in the trace | "It's reading papers, exploring the codebase, checking what data exists" |
| Data Engineering (Phase 2) | EDA notebook cells generating | "It found the UCI data in shared/, now computing RFM features" |
| Model Building (Phase 3) | `models/` directory filling up | "Training LightGBM regressor with log1p target — regression, not classification" |

**Key differences to call out vs. m03:**
- Regression, not classification → MAE/RMSE/Spearman instead of AUC
- BG/NBD baseline comparison (probabilistic model from `lifetimes` library)
- Temporal train/test split: observe 2009–2010, predict 2010–2011
- `/api/portfolio` endpoint (scatter plot) instead of `/api/segments`

---

### Block 6 — 0:50–1:00 | "Watching Agents Work + Q&A"

**Goal:** Let the skill run visibly while answering questions; close with a conviction summary.

**While agents are working:**
- Open a second terminal and do `ls m02_clv/` — show files appearing in real-time
- Show the `project_state.json` updating between phases
- Scroll through the Claude Code trace: "See the `Bash`, `Write`, `Read` tool calls? Every action is logged"

**Wrap-up talking points:**

1. **What just happened in 20 minutes:** Research brief drafted, data loaded, EDA notebook created, model training started — work that typically takes a data scientist 1–2 days
2. **What Claude Code is NOT:** It's not magic. It follows explicit patterns from the skill file and m01 reference. You review all the code it writes.
3. **How to go deeper:**
   - Read `~/.claude/skills/analytics-project.md` to understand the phases
   - Use `CLAUDE.md` in the repo to encode your project conventions
   - Skills are just markdown — write your own for your team's workflows
4. **Next step for the team:** Let m02 finish building in the background, review the artifacts tomorrow

**Closing line:** "One skill invocation. Research, notebooks, trained model, API, React app, documentation. That's Claude Code."

---

## Pre-Tutorial Checklist

- [ ] Claude Code installed: `npm install -g @anthropic/claude-code`
- [ ] API key set: `export ANTHROPIC_API_KEY=...`
- [ ] m03 node modules installed: `cd m03_churn/app/server && npm install && cd ../client && npm install`
- [ ] m02 stub directories exist (already in repo as `.gitkeep`)
- [ ] VSCode open at `/Users/aayan/MarketingAnalytics/` with m03 files visible
- [ ] UCI data confirmed: `ls shared/data/raw/` (should show xlsx + csv + zip)
- [ ] Python deps confirmed: `.venv/bin/pip show lightgbm lifetimes shap pyarrow`
- [ ] 3 terminals pre-opened and labeled: FastAPI / Express / Vite
- [ ] Browser pre-loaded to `http://localhost:5175` (start services 2 min before Block 4)

---

## Timing Buffer Notes

- Block 4 (demo) and Block 5 (live build) are the most likely to run over — have Block 6 Q&A as flex time
- If m03 services fail to start, use screenshots from a previous run as fallback
- The skill may take 20–45 minutes to complete all 5 phases — that's fine; the point is watching it start and understanding the pattern
