{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3 — Churn Propensity: Training & Evaluation\n",
    "\n",
    "This notebook trains, evaluates, and explains a **churn propensity model** for a telecommunications\n",
    "customer base (Telco Customer Churn dataset).  \n",
    "\n",
    "The pipeline follows these steps:\n",
    "\n",
    "1. Load engineered customer features (`customer_features.parquet`)\n",
    "2. Stratified 80/20 train/test split\n",
    "3. Cross-validate three models: Naive baseline, Logistic Regression, LightGBM\n",
    "4. Calibrate LightGBM probabilities with Isotonic Regression\n",
    "5. Evaluate: ROC, PR curves, confusion matrix\n",
    "6. Optimise decision threshold via a business cost function\n",
    "7. Top-decile lift analysis\n",
    "8. SHAP global feature importance\n",
    "9. Persist model artifacts\n",
    "\n",
    "---\n",
    "**Cost model:**  \n",
    "- False Negative (missed churner): **\\$200** per customer  \n",
    "- False Positive (unnecessary outreach): **\\$20** per customer  \n",
    "\n",
    "Churn prevalence ≈ 26.5%, so we weight positive examples with `scale_pos_weight = 2.77`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "os.environ.setdefault(\"LOKY_MAX_CPU_COUNT\", \"1\")\n",
    "os.environ.setdefault(\"MPLCONFIGDIR\", \"/tmp/mplconfig\")\n",
    "os.makedirs(os.environ[\"MPLCONFIGDIR\"], exist_ok=True)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import joblib\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    average_precision_score,\n",
    "    brier_score_loss,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_recall_curve,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    ConfusionMatrixDisplay,\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "\n",
    "# Add project root to path so src imports work\n",
    "BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(\"__file__\")))\n",
    "if BASE_DIR not in sys.path:\n",
    "    sys.path.insert(0, BASE_DIR)\n",
    "\n",
    "from src.feature_engineering import FEATURE_COLUMNS\n",
    "\n",
    "print(f\"Python: {sys.version.split()[0]}\")\n",
    "print(f\"LightGBM: {lgb.__version__}\")\n",
    "print(f\"SHAP: {shap.__version__}\")\n",
    "print(f\"Feature columns ({len(FEATURE_COLUMNS)}): OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Features and Inspect Class Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Paths\n",
    "# ---------------------------------------------------------------------------\n",
    "CUSTOMER_FEATURES_PARQUET = os.path.join(BASE_DIR, \"data\", \"processed\", \"customer_features.parquet\")\n",
    "MODELS_DIR = os.path.join(BASE_DIR, \"models\")\n",
    "DOCS_DIR = os.path.join(BASE_DIR, \"docs\")\n",
    "\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "os.makedirs(DOCS_DIR, exist_ok=True)\n",
    "\n",
    "TARGET_COLUMN = \"Churn\"\n",
    "CUSTOMER_ID_COLUMN = \"customerID\"\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Load\n",
    "# ---------------------------------------------------------------------------\n",
    "df = pd.read_parquet(CUSTOMER_FEATURES_PARQUET, engine=\"pyarrow\")\n",
    "print(f\"Loaded: {df.shape[0]:,} rows x {df.shape[1]} columns\")\n",
    "\n",
    "X = df[FEATURE_COLUMNS].copy()\n",
    "y = df[TARGET_COLUMN].astype(int).copy()\n",
    "customer_ids = df[CUSTOMER_ID_COLUMN].copy()\n",
    "\n",
    "print(f\"\\nX shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print()\n",
    "\n",
    "# Class balance\n",
    "class_counts = y.value_counts().sort_index()\n",
    "print(\"Class distribution:\")\n",
    "print(f\"  No Churn (0): {class_counts[0]:,}  ({class_counts[0]/len(y):.1%})\")\n",
    "print(f\"  Churn    (1): {class_counts[1]:,}  ({class_counts[1]/len(y):.1%})\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 3.5))\n",
    "bars = ax.bar([\"No Churn\", \"Churn\"], class_counts.values,\n",
    "               color=[\"#4C72B0\", \"#DD8452\"], edgecolor=\"white\", linewidth=0.8)\n",
    "for bar, count in zip(bars, class_counts.values):\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 20,\n",
    "            f\"{count:,}\\n({count/len(y):.1%})\", ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "ax.set_title(\"Class Balance — Churn Target\", fontsize=12)\n",
    "ax.set_ylabel(\"Customers\")\n",
    "ax.set_ylim(0, class_counts.max() * 1.18)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train/Test Split with Class Distribution Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.20\n",
    "\n",
    "X_train, X_test, y_train, y_test, ids_train, ids_test = train_test_split(\n",
    "    X, y, customer_ids,\n",
    "    test_size=TEST_SIZE,\n",
    "    stratify=y,\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "\n",
    "# Reset indices for clean iloc access later\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_test  = X_test.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_test  = y_test.reset_index(drop=True)\n",
    "ids_train = ids_train.reset_index(drop=True)\n",
    "ids_test  = ids_test.reset_index(drop=True)\n",
    "\n",
    "print(f\"Train: {len(X_train):,} rows  |  Test: {len(X_test):,} rows\")\n",
    "print(f\"Train churn rate: {y_train.mean():.1%}   Test churn rate: {y_test.mean():.1%}\")\n",
    "\n",
    "# Side-by-side bar chart\n",
    "fig, axes = plt.subplots(1, 2, figsize=(9, 3.5))\n",
    "for ax, y_split, title in zip(axes, [y_train, y_test], [\"Train set\", \"Test set\"]):\n",
    "    counts = y_split.value_counts().sort_index()\n",
    "    ax.bar([\"No Churn\", \"Churn\"], counts.values,\n",
    "           color=[\"#4C72B0\", \"#DD8452\"], edgecolor=\"white\", linewidth=0.8)\n",
    "    for i, (v, pct) in enumerate(zip(counts.values, counts.values / len(y_split))):\n",
    "        ax.text(i, v + 10, f\"{v:,}\\n({pct:.1%})\", ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel(\"Customers\")\n",
    "    ax.set_ylim(0, counts.max() * 1.18)\n",
    "plt.suptitle(\"Class Distribution by Split\", fontsize=12, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Baseline Models and Cross-Validated AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FOLDS = 5\n",
    "SCALE_POS_WEIGHT = 2.77  # 73.5 / 26.5\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "naive_prob = float(y_train.mean())\n",
    "\n",
    "naive_cv, lr_cv, lgbm_cv = [], [], []\n",
    "\n",
    "for fold_i, (tr_idx, vl_idx) in enumerate(skf.split(X_train, y_train), 1):\n",
    "    X_tr, X_vl = X_train.iloc[tr_idx], X_train.iloc[vl_idx]\n",
    "    y_tr, y_vl = y_train.iloc[tr_idx], y_train.iloc[vl_idx]\n",
    "\n",
    "    # Naive\n",
    "    naive_cv.append(roc_auc_score(y_vl, np.full(len(y_vl), naive_prob)))\n",
    "\n",
    "    # Logistic Regression\n",
    "    lr_fold = LogisticRegression(C=1.0, max_iter=1000, random_state=RANDOM_STATE)\n",
    "    lr_fold.fit(X_tr, y_tr)\n",
    "    lr_cv.append(roc_auc_score(y_vl, lr_fold.predict_proba(X_vl)[:, 1]))\n",
    "\n",
    "    # LightGBM\n",
    "    lgbm_fold = lgb.LGBMClassifier(\n",
    "        objective=\"binary\", metric=\"auc\",\n",
    "        learning_rate=0.05, num_leaves=31,\n",
    "        min_child_samples=20, feature_fraction=0.8,\n",
    "        bagging_fraction=0.8, bagging_freq=5,\n",
    "        scale_pos_weight=SCALE_POS_WEIGHT,\n",
    "        n_estimators=300, random_state=RANDOM_STATE, verbose=-1,\n",
    "    )\n",
    "    lgbm_fold.fit(X_tr, y_tr)\n",
    "    lgbm_cv.append(roc_auc_score(y_vl, lgbm_fold.predict_proba(X_vl)[:, 1]))\n",
    "\n",
    "    print(f\"  Fold {fold_i}: Naive={naive_cv[-1]:.4f}  LR={lr_cv[-1]:.4f}  LGBM={lgbm_cv[-1]:.4f}\")\n",
    "\n",
    "print()\n",
    "cv_summary = pd.DataFrame({\n",
    "    \"Model\": [\"Naive Baseline\", \"Logistic Regression\", \"LightGBM\"],\n",
    "    \"Mean CV AUC\": [np.mean(naive_cv), np.mean(lr_cv), np.mean(lgbm_cv)],\n",
    "    \"Std CV AUC\":  [np.std(naive_cv),  np.std(lr_cv),  np.std(lgbm_cv)],\n",
    "})\n",
    "cv_summary[\"Mean CV AUC\"] = cv_summary[\"Mean CV AUC\"].map(\"{:.4f}\".format)\n",
    "cv_summary[\"Std CV AUC\"]  = cv_summary[\"Std CV AUC\"].map(\"{:.4f}\".format)\n",
    "print(cv_summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. LightGBM Final Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Hyperparameter configuration — edit here to experiment\n",
    "# ============================================================\n",
    "LGBM_PARAMS = dict(\n",
    "    objective=\"binary\",\n",
    "    metric=\"auc\",\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=31,\n",
    "    min_child_samples=20,\n",
    "    feature_fraction=0.8,\n",
    "    bagging_fraction=0.8,\n",
    "    bagging_freq=5,\n",
    "    scale_pos_weight=SCALE_POS_WEIGHT,\n",
    "    n_estimators=300,\n",
    "    random_state=RANDOM_STATE,\n",
    "    verbose=-1,\n",
    ")\n",
    "# ============================================================\n",
    "\n",
    "print(\"Training LightGBM on full training set...\")\n",
    "print(f\"  Params: {LGBM_PARAMS}\")\n",
    "\n",
    "lgbm_model = lgb.LGBMClassifier(**LGBM_PARAMS)\n",
    "lgbm_model.fit(X_train, y_train)\n",
    "\n",
    "# Also train logistic regression final model\n",
    "lr_model = LogisticRegression(C=1.0, max_iter=1000, random_state=RANDOM_STATE)\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "print(f\"  LightGBM trained: {lgbm_model.n_estimators_} trees\")\n",
    "print(f\"  Logistic Regression trained: {X_train.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Probability Calibration with Isotonic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Internal 80/20 split of training set for calibration\n",
    "X_fit, X_cal, y_fit, y_cal = train_test_split(\n",
    "    X_train, y_train,\n",
    "    test_size=0.20,\n",
    "    stratify=y_train,\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "\n",
    "# Refit LightGBM on 80% portion\n",
    "lgbm_cal_inner = lgb.LGBMClassifier(**LGBM_PARAMS)\n",
    "lgbm_cal_inner.fit(X_fit, y_fit)\n",
    "\n",
    "# Score calibration hold-out\n",
    "cal_raw = lgbm_cal_inner.predict_proba(X_cal)[:, 1]\n",
    "\n",
    "# Fit isotonic regression\n",
    "calibrator = IsotonicRegression(y_min=0.0, y_max=1.0, out_of_bounds=\"clip\")\n",
    "calibrator.fit(cal_raw, y_cal)\n",
    "\n",
    "print(f\"Calibration: fit={len(X_fit):,} rows, cal={len(X_cal):,} rows\")\n",
    "print(f\"Calibrator type: {type(calibrator).__name__}\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Plot: uncalibrated vs calibrated probability curves\n",
    "# ---------------------------------------------------------------------------\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "# Get test set scores from the FULL model (trained on all X_train)\n",
    "test_raw_scores = lgbm_model.predict_proba(X_test)[:, 1]\n",
    "test_cal_scores = calibrator.predict(test_raw_scores)\n",
    "\n",
    "frac_pos_raw, mean_pred_raw = calibration_curve(y_test, test_raw_scores, n_bins=10)\n",
    "frac_pos_cal, mean_pred_cal = calibration_curve(y_test, test_cal_scores, n_bins=10)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "ax.plot([0, 1], [0, 1], \"k--\", linewidth=1, label=\"Perfect calibration\")\n",
    "ax.plot(mean_pred_raw, frac_pos_raw, \"o-\", color=\"#DD8452\", label=\"LightGBM (uncalibrated)\", markersize=5)\n",
    "ax.plot(mean_pred_cal, frac_pos_cal, \"s-\", color=\"#4C72B0\", label=\"LightGBM (isotonic)\", markersize=5)\n",
    "ax.set_xlabel(\"Mean Predicted Probability\")\n",
    "ax.set_ylabel(\"Fraction of Positives\")\n",
    "ax.set_title(\"Calibration Curve — Uncalibrated vs Isotonic\")\n",
    "ax.legend()\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation: ROC Curve, PR Curve, Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score all models on test set\n",
    "naive_scores  = np.full(len(y_test), float(y_train.mean()))\n",
    "lr_scores     = lr_model.predict_proba(X_test)[:, 1]\n",
    "lgbm_raw      = lgbm_model.predict_proba(X_test)[:, 1]\n",
    "lgbm_scores   = calibrator.predict(lgbm_raw)\n",
    "\n",
    "auc_naive = roc_auc_score(y_test, naive_scores)\n",
    "auc_lr    = roc_auc_score(y_test, lr_scores)\n",
    "auc_lgbm  = roc_auc_score(y_test, lgbm_scores)\n",
    "\n",
    "pr_auc_lgbm = average_precision_score(y_test, lgbm_scores)\n",
    "brier_lgbm  = brier_score_loss(y_test, lgbm_scores)\n",
    "\n",
    "print(f\"AUC-ROC  — Naive: {auc_naive:.4f} | LR: {auc_lr:.4f} | LightGBM (cal): {auc_lgbm:.4f}\")\n",
    "print(f\"PR-AUC   — LightGBM: {pr_auc_lgbm:.4f}\")\n",
    "print(f\"Brier    — LightGBM: {brier_lgbm:.6f}\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# ROC Curve\n",
    "# ---------------------------------------------------------------------------\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "ax = axes[0]\n",
    "for scores, label, color in [\n",
    "    (naive_scores, f\"Naive (AUC={auc_naive:.3f})\",  \"#929591\"),\n",
    "    (lr_scores,    f\"Logistic (AUC={auc_lr:.3f})\",   \"#DD8452\"),\n",
    "    (lgbm_scores,  f\"LightGBM (AUC={auc_lgbm:.3f})\", \"#4C72B0\"),\n",
    "]:\n",
    "    fpr, tpr, _ = roc_curve(y_test, scores)\n",
    "    ax.plot(fpr, tpr, label=label, linewidth=2, color=color)\n",
    "ax.plot([0, 1], [0, 1], \"k--\", linewidth=1)\n",
    "ax.set_xlabel(\"False Positive Rate\")\n",
    "ax.set_ylabel(\"True Positive Rate\")\n",
    "ax.set_title(\"ROC Curves\")\n",
    "ax.legend(fontsize=9)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# PR Curve\n",
    "# ---------------------------------------------------------------------------\n",
    "ax = axes[1]\n",
    "prec, rec, _ = precision_recall_curve(y_test, lgbm_scores)\n",
    "ax.plot(rec, prec, color=\"#4C72B0\", linewidth=2, label=f\"LightGBM (PR-AUC={pr_auc_lgbm:.3f})\")\n",
    "ax.axhline(y_test.mean(), color=\"#929591\", linestyle=\"--\", linewidth=1, label=f\"Baseline ({y_test.mean():.3f})\")\n",
    "ax.set_xlabel(\"Recall\")\n",
    "ax.set_ylabel(\"Precision\")\n",
    "ax.set_title(\"Precision-Recall Curve — LightGBM (calibrated)\")\n",
    "ax.legend(fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Confusion Matrix at 0.5 threshold\n",
    "# ---------------------------------------------------------------------------\n",
    "y_pred_05 = (lgbm_scores >= 0.5).astype(int)\n",
    "print(f\"\\nMetrics @ threshold 0.5:\")\n",
    "print(f\"  F1:        {f1_score(y_test, y_pred_05):.4f}\")\n",
    "print(f\"  Precision: {precision_score(y_test, y_pred_05):.4f}\")\n",
    "print(f\"  Recall:    {recall_score(y_test, y_pred_05):.4f}\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "cm = confusion_matrix(y_test, y_pred_05)\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels=[\"No Churn\", \"Churn\"])\n",
    "disp.plot(ax=ax, colorbar=False, cmap=\"Blues\")\n",
    "ax.set_title(\"Confusion Matrix @ threshold 0.5\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Threshold Optimisation: Cost Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COST_FN = 200   # missed churner\n",
    "COST_FP = 20    # false alarm\n",
    "\n",
    "thresholds = np.linspace(0.1, 0.9, 81)\n",
    "costs = []\n",
    "f1s = []\n",
    "\n",
    "for thr in thresholds:\n",
    "    y_pred = (lgbm_scores >= thr).astype(int)\n",
    "    cm_thr = confusion_matrix(y_test, y_pred)\n",
    "    fn = cm_thr[1, 0] if cm_thr.shape == (2, 2) else 0\n",
    "    fp = cm_thr[0, 1] if cm_thr.shape == (2, 2) else 0\n",
    "    costs.append(fn * COST_FN + fp * COST_FP)\n",
    "    f1s.append(f1_score(y_test, y_pred, zero_division=0))\n",
    "\n",
    "costs = np.array(costs)\n",
    "f1s   = np.array(f1s)\n",
    "\n",
    "best_idx = int(np.argmin(costs))\n",
    "cost_optimal_threshold = float(thresholds[best_idx])\n",
    "cost_optimal_value     = float(costs[best_idx])\n",
    "\n",
    "print(f\"Cost-optimal threshold: {cost_optimal_threshold:.2f}\")\n",
    "print(f\"Total cost at optimal:  ${cost_optimal_value:,.0f}\")\n",
    "print(f\"F1 at optimal:          {f1s[best_idx]:.4f}\")\n",
    "\n",
    "# Metrics at optimal threshold\n",
    "y_pred_opt = (lgbm_scores >= cost_optimal_threshold).astype(int)\n",
    "print(f\"Precision:              {precision_score(y_test, y_pred_opt):.4f}\")\n",
    "print(f\"Recall:                 {recall_score(y_test, y_pred_opt):.4f}\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Cost curve plot\n",
    "# ---------------------------------------------------------------------------\n",
    "fig, ax1 = plt.subplots(figsize=(8, 4.5))\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "ax1.plot(thresholds, costs, color=\"#DD8452\", linewidth=2, label=\"Total cost ($)\")\n",
    "ax1.axvline(cost_optimal_threshold, color=\"red\", linestyle=\"--\", linewidth=1.5,\n",
    "            label=f\"Optimal threshold = {cost_optimal_threshold:.2f}\")\n",
    "ax1.scatter([cost_optimal_threshold], [cost_optimal_value], color=\"red\", s=80, zorder=5)\n",
    "ax1.set_xlabel(\"Decision Threshold\")\n",
    "ax1.set_ylabel(\"Total Cost ($)\", color=\"#DD8452\")\n",
    "ax1.tick_params(axis=\"y\", labelcolor=\"#DD8452\")\n",
    "\n",
    "ax2.plot(thresholds, f1s, color=\"#4C72B0\", linewidth=1.5, linestyle=\":\", label=\"F1 score\")\n",
    "ax2.set_ylabel(\"F1 Score\", color=\"#4C72B0\")\n",
    "ax2.tick_params(axis=\"y\", labelcolor=\"#4C72B0\")\n",
    "\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, fontsize=9, loc=\"upper right\")\n",
    "\n",
    "ax1.set_title(f\"Business Cost vs. Decision Threshold\\n\"\n",
    "              f\"(FN=${COST_FN}/customer, FP=${COST_FP}/customer)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Top-Decile Lift Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign score deciles (1 = top-scored, 10 = lowest-scored)\n",
    "decile_labels = pd.qcut(\n",
    "    lgbm_scores,\n",
    "    q=10,\n",
    "    labels=[f\"D{i}\" for i in range(1, 11)],\n",
    "    duplicates=\"drop\",\n",
    ")\n",
    "decile_df = pd.DataFrame({\n",
    "    \"decile\": decile_labels,\n",
    "    \"churn\": y_test.values,\n",
    "    \"score\": lgbm_scores,\n",
    "})\n",
    "\n",
    "# Sort deciles so D1 = highest-scored (most likely to churn)\n",
    "decile_order = [f\"D{i}\" for i in range(1, 11)]\n",
    "decile_stats = (\n",
    "    decile_df.groupby(\"decile\", observed=True)\n",
    "    .agg(churn_rate=(\"churn\", \"mean\"), n=(\"churn\", \"count\"))\n",
    "    .reindex(decile_order)\n",
    "    .iloc[::-1]  # flip so highest-score (D10 from qcut) is on the left\n",
    "    .reset_index()\n",
    ")\n",
    "decile_stats.index = [f\"D{i}\" for i in range(1, len(decile_stats) + 1)]\n",
    "\n",
    "overall_churn_rate = float(y_test.mean())\n",
    "lift = decile_stats[\"churn_rate\"] / overall_churn_rate\n",
    "\n",
    "print(\"Top-decile lift table:\")\n",
    "print(pd.DataFrame({\"Decile\": decile_stats.index,\n",
    "                    \"Churn Rate\": decile_stats[\"churn_rate\"].map(\"{:.1%}\".format),\n",
    "                    \"Lift vs. Mean\": lift.map(\"{:.2f}x\".format)}).to_string(index=False))\n",
    "\n",
    "# Capture rate: top-20% = deciles 1-2\n",
    "top2_decile_rows = decile_df[decile_df[\"decile\"].isin(decile_order[-2:])]\n",
    "top20_capture = top2_decile_rows[\"churn\"].sum() / y_test.sum()\n",
    "print(f\"\\nTop-20% capture rate: {top20_capture:.1%} of all churners\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 4.5))\n",
    "bars = ax.bar(\n",
    "    decile_stats.index,\n",
    "    decile_stats[\"churn_rate\"],\n",
    "    color=[\"#DD8452\" if r >= overall_churn_rate * 1.5 else \"#4C72B0\" for r in decile_stats[\"churn_rate\"]],\n",
    "    edgecolor=\"white\", linewidth=0.8,\n",
    ")\n",
    "ax.axhline(overall_churn_rate, color=\"black\", linestyle=\"--\", linewidth=1.5, label=f\"Overall rate ({overall_churn_rate:.1%})\")\n",
    "for bar, rate in zip(bars, decile_stats[\"churn_rate\"]):\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.003,\n",
    "            f\"{rate:.0%}\", ha=\"center\", va=\"bottom\", fontsize=8)\n",
    "ax.set_xlabel(\"Predicted Score Decile (D1 = highest-scored)\")\n",
    "ax.set_ylabel(\"Actual Churn Rate\")\n",
    "ax.set_title(\"Top-Decile Lift Chart — Actual Churn Rate by Score Decile\")\n",
    "ax.legend(fontsize=9)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. SHAP Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Computing SHAP values on test set (using 200-row background)...\")\n",
    "bg_sample = X_train.sample(min(200, len(X_train)), random_state=RANDOM_STATE)\n",
    "explainer = shap.TreeExplainer(lgbm_model, data=bg_sample)\n",
    "\n",
    "# SHAP values on test set (for display)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "print(f\"SHAP values shape: {np.array(shap_values).shape}\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Global beeswarm plot\n",
    "# ---------------------------------------------------------------------------\n",
    "print(\"\\nBeeswarm plot (global feature impact):\")\n",
    "shap.summary_plot(\n",
    "    shap_values,\n",
    "    X_test,\n",
    "    plot_type=\"dot\",\n",
    "    max_display=20,\n",
    "    show=True,\n",
    ")\n",
    "plt.title(\"SHAP Beeswarm — Global Feature Importance\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Top-10 feature importance bar chart (mean |SHAP|)\n",
    "# ---------------------------------------------------------------------------\n",
    "mean_abs_shap = np.abs(shap_values).mean(axis=0)\n",
    "feature_importance = pd.Series(mean_abs_shap, index=FEATURE_COLUMNS).sort_values(ascending=False)\n",
    "\n",
    "top10 = feature_importance.head(10)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4.5))\n",
    "ax.barh(top10.index[::-1], top10.values[::-1], color=\"#4C72B0\", edgecolor=\"white\", linewidth=0.8)\n",
    "ax.set_xlabel(\"Mean |SHAP value|\")\n",
    "ax.set_title(\"Top-10 Features by Mean Absolute SHAP Value\")\n",
    "for i, (val, name) in enumerate(zip(top10.values[::-1], top10.index[::-1])):\n",
    "    ax.text(val + 0.001, i, f\"{val:.4f}\", va=\"center\", fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Top-10 features by mean |SHAP|:\")\n",
    "print(top10.map(\"{:.4f}\".format).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "os.makedirs(DOCS_DIR, exist_ok=True)\n",
    "\n",
    "MODEL_ARTIFACT     = os.path.join(MODELS_DIR, \"lgbm_churn.pkl\")\n",
    "CALIBRATOR_ARTIFACT = os.path.join(MODELS_DIR, \"probability_calibrator.pkl\")\n",
    "SHAP_ARTIFACT      = os.path.join(MODELS_DIR, \"shap_explainer.pkl\")\n",
    "METADATA_PATH      = os.path.join(MODELS_DIR, \"metadata.json\")\n",
    "\n",
    "# Save LightGBM model\n",
    "joblib.dump(lgbm_model, MODEL_ARTIFACT)\n",
    "print(f\"Saved: {MODEL_ARTIFACT}\")\n",
    "\n",
    "# Save calibrator\n",
    "joblib.dump(calibrator, CALIBRATOR_ARTIFACT)\n",
    "print(f\"Saved: {CALIBRATOR_ARTIFACT}\")\n",
    "\n",
    "# Save SHAP explainer\n",
    "joblib.dump(explainer, SHAP_ARTIFACT)\n",
    "print(f\"Saved: {SHAP_ARTIFACT}\")\n",
    "\n",
    "# Save metadata.json\n",
    "metadata = {\n",
    "    \"feature_columns\": FEATURE_COLUMNS,\n",
    "    \"train_rows\": int(len(X_train)),\n",
    "    \"test_rows\": int(len(X_test)),\n",
    "    \"metrics\": {\n",
    "        \"auc_naive\":   float(auc_naive),\n",
    "        \"auc_lr\":      float(auc_lr),\n",
    "        \"auc_lgbm\":    float(auc_lgbm),\n",
    "        \"pr_auc_lgbm\": float(pr_auc_lgbm),\n",
    "        \"brier_lgbm\":  float(brier_lgbm),\n",
    "        \"f1_05\":       float(f1_score(y_test, (lgbm_scores >= 0.5).astype(int))),\n",
    "        \"precision_05\": float(precision_score(y_test, (lgbm_scores >= 0.5).astype(int))),\n",
    "        \"recall_05\":   float(recall_score(y_test, (lgbm_scores >= 0.5).astype(int))),\n",
    "        \"top20_lift\":  float(top20_capture),\n",
    "    },\n",
    "    \"cost_optimal_threshold\": float(cost_optimal_threshold),\n",
    "    \"timestamp\": datetime.now(timezone.utc).isoformat(),\n",
    "}\n",
    "with open(METADATA_PATH, \"w\", encoding=\"utf-8\") as fh:\n",
    "    json.dump(metadata, fh, indent=2)\n",
    "print(f\"Saved: {METADATA_PATH}\")\n",
    "\n",
    "print(\"\\n--- Metadata summary ---\")\n",
    "print(json.dumps({k: v for k, v in metadata.items() if k != \"feature_columns\"}, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
