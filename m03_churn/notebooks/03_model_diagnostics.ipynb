{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0001-4000-8000-000000000001",
   "metadata": {},
   "source": [
    "# Model Diagnostics — Churn Propensity Model 3\n",
    "\n",
    "**Intent:** Comprehensive post-training audit of the LightGBM binary churn classifier across six diagnostic lenses:\n",
    "threshold sensitivity, SHAP explainability, business simulation, slice analysis, failure modes, and score distribution.\n",
    "\n",
    "This notebook is read-only with respect to the saved artifacts. It loads trained models and the held-out test set,\n",
    "then produces plots and tables that inform threshold selection and deployment decisions.\n",
    "\n",
    "**Sections:**\n",
    "1. Threshold Analysis — confusion matrices and precision/recall trade-off at default vs. cost-optimized threshold\n",
    "2. SHAP Analysis — global feature importance and individual customer explanations\n",
    "3. Business Simulation — gain curve, intervention cost vs. revenue saved, break-even analysis\n",
    "4. Slice Analysis — AUC across Contract type, InternetService, and SeniorCitizen subgroups\n",
    "5. Failure Mode Analysis — profiling false negatives and false positives\n",
    "6. Score Distribution — predicted probability histograms and KS statistic\n",
    "\n",
    "**Inputs:**\n",
    "- `data/processed/customer_features.parquet` — full feature matrix (7,043 rows, 35 columns)\n",
    "- `models/lgbm_churn.pkl` — trained LightGBM binary classifier\n",
    "- `models/probability_calibrator.pkl` — isotonic regression calibrator\n",
    "- `models/shap_explainer.pkl` — pre-computed TreeExplainer\n",
    "- `models/metadata.json` — training run metadata including cost-optimized threshold\n",
    "\n",
    "**References:**\n",
    "- Ke, G. et al. (2017). *LightGBM: A Highly Efficient Gradient Boosting Decision Tree*. NeurIPS.\n",
    "- Lundberg, S. & Lee, S.-I. (2017). *A Unified Approach to Interpreting Model Predictions*. NeurIPS.\n",
    "- Fawcett, T. (2006). *An introduction to ROC analysis*. Pattern Recognition Letters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0002-4000-8000-000000000002",
   "metadata": {},
   "source": [
    "## 0. Imports and Load Artifacts\n",
    "\n",
    "**Intent:** Load all model artifacts and reconstruct the 20% stratified holdout test set\n",
    "using the same random seed as training so results are reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0003-4000-8000-000000000003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report,\n",
    "    roc_auc_score, precision_score, recall_score, f1_score,\n",
    "    roc_curve, ks_2samp,\n",
    ")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', 60)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Paths — all relative to this notebook's parent directory (m03_churn/)\n",
    "# ---------------------------------------------------------------------------\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "MODEL_DIR   = PROJECT_ROOT / 'models'\n",
    "DATA_DIR    = PROJECT_ROOT / 'data' / 'processed'\n",
    "\n",
    "FEATURES_PATH    = DATA_DIR  / 'customer_features.parquet'\n",
    "MODEL_PATH       = MODEL_DIR / 'lgbm_churn.pkl'\n",
    "CALIBRATOR_PATH  = MODEL_DIR / 'probability_calibrator.pkl'\n",
    "EXPLAINER_PATH   = MODEL_DIR / 'shap_explainer.pkl'\n",
    "METADATA_PATH    = MODEL_DIR / 'metadata.json'\n",
    "\n",
    "for p in [FEATURES_PATH, MODEL_PATH, CALIBRATOR_PATH, EXPLAINER_PATH, METADATA_PATH]:\n",
    "    assert p.exists(), f'Missing required artifact: {p}'\n",
    "\n",
    "print('All artifacts found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0004-4000-8000-000000000004",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.feature_engineering import FEATURE_COLUMNS\n",
    "\n",
    "# Load artifacts\n",
    "model      = joblib.load(MODEL_PATH)\n",
    "calibrator = joblib.load(CALIBRATOR_PATH)\n",
    "explainer  = joblib.load(EXPLAINER_PATH)\n",
    "\n",
    "with open(METADATA_PATH) as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "# Cost-optimized threshold stored in metadata by the training script\n",
    "COST_THRESHOLD = float(metadata.get('cost_optimized_threshold', 0.40))\n",
    "DEFAULT_THRESHOLD = 0.50\n",
    "\n",
    "print('Model type:              ', type(model).__name__)\n",
    "print('Calibrator type:         ', type(calibrator).__name__)\n",
    "print('Feature columns (32):    ', len(FEATURE_COLUMNS))\n",
    "print('Default threshold:       ', DEFAULT_THRESHOLD)\n",
    "print('Cost-optimized threshold:', COST_THRESHOLD)\n",
    "print()\n",
    "print('Metadata:')\n",
    "for k, v in metadata.items():\n",
    "    print(f'  {k}: {v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0005-4000-8000-000000000005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load full feature matrix and reconstruct the 20% stratified holdout\n",
    "# Uses the same seed (42) as the training script to guarantee identical split\n",
    "df_full = pd.read_parquet(FEATURES_PATH)\n",
    "\n",
    "X_all = df_full[FEATURE_COLUMNS]\n",
    "y_all = df_full['Churn'].astype(int)\n",
    "\n",
    "_, X_test, _, y_test = train_test_split(\n",
    "    X_all, y_all,\n",
    "    test_size=0.20,\n",
    "    stratify=y_all,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Reconstruct the test-set customer metadata for slice and failure mode analysis\n",
    "df_test = df_full.loc[X_test.index].copy()\n",
    "\n",
    "# Generate calibrated probability scores on the test set\n",
    "raw_proba  = model.predict_proba(X_test)[:, 1]\n",
    "cal_proba  = calibrator.predict(raw_proba)   # isotonic calibration\n",
    "df_test['pred_proba'] = cal_proba\n",
    "\n",
    "print(f'Test set:    {X_test.shape[0]:,} rows  |  churn rate: {y_test.mean():.1%}')\n",
    "print(f'Score range: [{cal_proba.min():.3f}, {cal_proba.max():.3f}]  |  mean: {cal_proba.mean():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0006-4000-8000-000000000006",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1 — Threshold Analysis\n",
    "\n",
    "**Intent:** Compare model performance at the naive 0.5 default against a cost-optimized threshold\n",
    "derived from the telecom context: a false negative (missed churner) costs roughly 10× more\n",
    "in lost revenue than a false positive (wasted retention contact).\n",
    "\n",
    "The cost-optimized threshold minimises `FN_cost × FN + FP_cost × FP` over the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0007-4000-8000-000000000007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_metrics(y_true, proba, threshold):\n",
    "    \"\"\"Return classification metrics at a given decision threshold.\"\"\"\n",
    "    y_pred = (proba >= threshold).astype(int)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    return {\n",
    "        'threshold':  threshold,\n",
    "        'precision':  precision_score(y_true, y_pred, zero_division=0),\n",
    "        'recall':     recall_score(y_true, y_pred, zero_division=0),\n",
    "        'f1':         f1_score(y_true, y_pred, zero_division=0),\n",
    "        'auc':        roc_auc_score(y_true, proba),\n",
    "        'cm':         cm,\n",
    "    }\n",
    "\n",
    "metrics_default = threshold_metrics(y_test, cal_proba, DEFAULT_THRESHOLD)\n",
    "metrics_cost    = threshold_metrics(y_test, cal_proba, COST_THRESHOLD)\n",
    "\n",
    "print(f'AUC (threshold-independent): {metrics_default[\"auc\"]:.4f}')\n",
    "print()\n",
    "for label, m in [('Default (0.50)', metrics_default), (f'Cost-optimized ({COST_THRESHOLD:.2f})', metrics_cost)]:\n",
    "    print(f'{label}')\n",
    "    print(f'  Precision: {m[\"precision\"]:.4f}   Recall: {m[\"recall\"]:.4f}   F1: {m[\"f1\"]:.4f}')\n",
    "    print(f'  Confusion matrix:\\n{m[\"cm\"]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0008-4000-8000-000000000008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(ax, cm, threshold_label, cmap='Blues'):\n",
    "    \"\"\"Draw a labelled heatmap of a 2×2 confusion matrix on the given Axes.\"\"\"\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "\n",
    "    classes = ['Not Churned\\n(0)', 'Churned\\n(1)']\n",
    "    tick_marks = [0, 1]\n",
    "    ax.set_xticks(tick_marks)\n",
    "    ax.set_xticklabels(classes, fontsize=9)\n",
    "    ax.set_yticks(tick_marks)\n",
    "    ax.set_yticklabels(classes, fontsize=9)\n",
    "\n",
    "    # Annotate each cell with the raw count and percentage of total\n",
    "    total = cm.sum()\n",
    "    thresh = cm.max() / 2.0\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            count = cm[i, j]\n",
    "            pct   = count / total * 100\n",
    "            color = 'white' if cm[i, j] > thresh else 'black'\n",
    "            ax.text(j, i, f'{count:,}\\n({pct:.1f}%)',\n",
    "                    ha='center', va='center', color=color, fontsize=10, fontweight='bold')\n",
    "\n",
    "    ax.set_xlabel('Predicted Label', fontsize=10)\n",
    "    ax.set_ylabel('True Label', fontsize=10)\n",
    "    ax.set_title(f'Confusion Matrix\\nThreshold = {threshold_label}', fontsize=11)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(11, 4))\n",
    "plot_confusion_matrix(axes[0], metrics_default['cm'], threshold_label='0.50 (default)')\n",
    "plot_confusion_matrix(axes[1], metrics_cost['cm'],    threshold_label=f'{COST_THRESHOLD:.2f} (cost-optimized)', cmap='Oranges')\n",
    "fig.suptitle('Confusion Matrix Comparison — Test Set', fontsize=13, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0009-4000-8000-000000000009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side bar chart: precision, recall, F1 at both thresholds\n",
    "metrics_labels = ['Precision', 'Recall', 'F1']\n",
    "default_vals   = [metrics_default['precision'], metrics_default['recall'], metrics_default['f1']]\n",
    "cost_vals      = [metrics_cost['precision'],    metrics_cost['recall'],    metrics_cost['f1']]\n",
    "\n",
    "x     = np.arange(len(metrics_labels))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 4))\n",
    "bars_a = ax.bar(x - width / 2, default_vals, width, label='Default (0.50)',\n",
    "                color='steelblue', edgecolor='white')\n",
    "bars_b = ax.bar(x + width / 2, cost_vals,    width, label=f'Cost-optimized ({COST_THRESHOLD:.2f})',\n",
    "                color='darkorange', edgecolor='white')\n",
    "\n",
    "# Value labels on top of each bar\n",
    "for bars in [bars_a, bars_b]:\n",
    "    for bar in bars:\n",
    "        h = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width() / 2, h + 0.01, f'{h:.3f}',\n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics_labels, fontsize=11)\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title(f'Precision / Recall / F1 at Two Thresholds  (AUC = {metrics_default[\"auc\"]:.4f})')\n",
    "ax.legend(fontsize=9)\n",
    "ax.axhline(0.5, color='gray', linestyle='--', linewidth=0.8, alpha=0.6)\n",
    "ax.yaxis.set_minor_locator(mticker.MultipleLocator(0.05))\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary table\n",
    "comparison_df = pd.DataFrame([\n",
    "    {'threshold': DEFAULT_THRESHOLD, 'precision': metrics_default['precision'],\n",
    "     'recall': metrics_default['recall'], 'f1': metrics_default['f1']},\n",
    "    {'threshold': COST_THRESHOLD,    'precision': metrics_cost['precision'],\n",
    "     'recall': metrics_cost['recall'],    'f1': metrics_cost['f1']},\n",
    "])\n",
    "comparison_df['label'] = ['Default (0.50)', f'Cost-optimized ({COST_THRESHOLD:.2f})']\n",
    "comparison_df.set_index('label', inplace=True)\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0010-4000-8000-000000000010",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2 — SHAP Analysis\n",
    "\n",
    "**Intent:** Use SHAP (SHapley Additive exPlanations) to understand which features drive\n",
    "the model globally and to explain individual predictions for three representative customers:\n",
    "a high-risk churner (top 5%), a borderline case (45–55th percentile), and a low-risk customer (bottom 5%).\n",
    "\n",
    "TreeExplainer is exact for LightGBM models — no approximations needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0011-4000-8000-000000000011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute SHAP values on the full test set\n",
    "# explainer was serialised after fitting on training data so base_values are valid\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# LightGBM TreeExplainer returns a list [class0, class1] for binary classification\n",
    "# We use class-1 (churn) SHAP values throughout\n",
    "if isinstance(shap_values, list) and len(shap_values) == 2:\n",
    "    sv = shap_values[1]   # shape: (n_test, n_features)\n",
    "else:\n",
    "    sv = shap_values      # already class-1 if explainer returns 2-D array\n",
    "\n",
    "print(f'SHAP value array shape: {sv.shape}')\n",
    "print(f'Test set shape:         {X_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0012-4000-8000-000000000012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Global beeswarm plot — top 15 features ---\n",
    "# Each point is one test customer. Color encodes the raw feature value (red = high, blue = low).\n",
    "# X-axis shows the SHAP value: positive pushes toward churn=1, negative away from it.\n",
    "\n",
    "# Compute mean absolute SHAP per feature to identify top 15\n",
    "mean_abs_shap = np.abs(sv).mean(axis=0)\n",
    "top15_idx     = np.argsort(mean_abs_shap)[-15:]         # indices of top-15 by importance\n",
    "top15_names   = [FEATURE_COLUMNS[i] for i in top15_idx]\n",
    "top15_sv      = sv[:, top15_idx]                        # SHAP values for those features\n",
    "top15_Xvals   = X_test.values[:, top15_idx]             # raw feature values (for colouring)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 7))\n",
    "\n",
    "# Jitter y-positions so overlapping points are visible (beeswarm approximation)\n",
    "rng = np.random.default_rng(seed=42)\n",
    "\n",
    "for row_idx, (fname, col_sv, col_x) in enumerate(zip(top15_names, top15_sv.T, top15_Xvals.T)):\n",
    "    # Normalise feature values 0→1 for the colormap\n",
    "    x_norm = (col_x - col_x.min()) / (col_x.ptp() + 1e-9)\n",
    "    colors  = plt.cm.RdBu_r(x_norm)                      # red = high value, blue = low\n",
    "\n",
    "    # Small vertical jitter so points don't stack on the same y-line\n",
    "    y_jitter = row_idx + rng.uniform(-0.3, 0.3, size=len(col_sv))\n",
    "\n",
    "    ax.scatter(col_sv, y_jitter, c=colors, alpha=0.4, s=8, linewidths=0)\n",
    "\n",
    "ax.axvline(0, color='black', linewidth=0.8)\n",
    "ax.set_yticks(range(len(top15_names)))\n",
    "ax.set_yticklabels(top15_names, fontsize=9)\n",
    "ax.set_xlabel('SHAP value  (impact on log-odds of churn)', fontsize=10)\n",
    "ax.set_title('Global SHAP Beeswarm — Top 15 Features\\n(Red = high feature value, Blue = low)', fontsize=11)\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Colorbar legend\n",
    "sm = plt.cm.ScalarMappable(cmap='RdBu_r', norm=plt.Normalize(0, 1))\n",
    "sm.set_array([])\n",
    "cb = fig.colorbar(sm, ax=ax, fraction=0.03, pad=0.02)\n",
    "cb.set_label('Feature value\\n(normalised)', fontsize=8)\n",
    "cb.set_ticks([0, 1])\n",
    "cb.set_ticklabels(['Low', 'High'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0013-4000-8000-000000000013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Select three example customers for force plots ---\n",
    "# High-risk:   customers whose calibrated score is in the top 5%\n",
    "# Medium-risk: customers in the 45–55th percentile band\n",
    "# Low-risk:    customers in the bottom 5%\n",
    "\n",
    "scores = df_test['pred_proba'].values\n",
    "\n",
    "p05  = np.percentile(scores, 5)\n",
    "p45  = np.percentile(scores, 45)\n",
    "p55  = np.percentile(scores, 55)\n",
    "p95  = np.percentile(scores, 95)\n",
    "\n",
    "high_mask   = scores >= p95\n",
    "medium_mask = (scores >= p45) & (scores <= p55)\n",
    "low_mask    = scores <= p05\n",
    "\n",
    "# Pick the customer closest to the respective percentile boundary\n",
    "def pick_representative(mask, scores, target):\n",
    "    \"\"\"Return the positional index (into df_test) of the customer closest to target score.\"\"\"\n",
    "    candidates = np.where(mask)[0]\n",
    "    closest    = candidates[np.argmin(np.abs(scores[candidates] - target))]\n",
    "    return closest\n",
    "\n",
    "idx_high   = pick_representative(high_mask,   scores, p95)\n",
    "idx_medium = pick_representative(medium_mask, scores, (p45 + p55) / 2)\n",
    "idx_low    = pick_representative(low_mask,    scores, p05)\n",
    "\n",
    "examples = {\n",
    "    'High Risk (top 5%)':        idx_high,\n",
    "    'Medium Risk (45–55th pct)': idx_medium,\n",
    "    'Low Risk (bottom 5%)':      idx_low,\n",
    "}\n",
    "\n",
    "X_test_arr = X_test.values  # numpy array for indexing into sv\n",
    "test_index = X_test.reset_index(drop=True)  # reindexed for clean iloc access\n",
    "\n",
    "for label, idx in examples.items():\n",
    "    score = scores[idx]\n",
    "    true  = y_test.values[idx]\n",
    "    print(f'{label:30s}  score={score:.4f}  actual_churn={true}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0014-4000-8000-000000000014",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_force_bar(ax, shap_row, feature_names, score, customer_label, n_features=10):\n",
    "    \"\"\"\n",
    "    Draw a horizontal waterfall / force bar chart for a single customer.\n",
    "    Shows the top n_features SHAP contributors (positive and negative) sorted by magnitude.\n",
    "    \"\"\"\n",
    "    # Identify top contributors by absolute value\n",
    "    top_idx   = np.argsort(np.abs(shap_row))[-n_features:]\n",
    "    top_names = [feature_names[i] for i in top_idx]\n",
    "    top_vals  = shap_row[top_idx]\n",
    "\n",
    "    # Sort by SHAP value for display (most negative at bottom)\n",
    "    order      = np.argsort(top_vals)\n",
    "    top_names  = [top_names[i] for i in order]\n",
    "    top_vals   = top_vals[order]\n",
    "\n",
    "    colors = ['#d73027' if v > 0 else '#4575b4' for v in top_vals]  # red=push churn, blue=push retain\n",
    "    y_pos  = np.arange(len(top_vals))\n",
    "\n",
    "    ax.barh(y_pos, top_vals, color=colors, edgecolor='white', height=0.7)\n",
    "    ax.axvline(0, color='black', linewidth=0.8)\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(top_names, fontsize=8)\n",
    "    ax.set_xlabel('SHAP value', fontsize=8)\n",
    "    ax.set_title(f'{customer_label}\\nPredicted churn probability: {score:.3f}', fontsize=9)\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "    # Value labels\n",
    "    for i, v in enumerate(top_vals):\n",
    "        offset = 0.002 if v >= 0 else -0.002\n",
    "        align  = 'left' if v >= 0 else 'right'\n",
    "        ax.text(v + offset, i, f'{v:+.3f}', va='center', ha=align, fontsize=7)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for ax, (label, idx) in zip(axes, examples.items()):\n",
    "    plot_force_bar(\n",
    "        ax,\n",
    "        shap_row=sv[idx],\n",
    "        feature_names=FEATURE_COLUMNS,\n",
    "        score=scores[idx],\n",
    "        customer_label=label,\n",
    "        n_features=10,\n",
    "    )\n",
    "\n",
    "# Shared legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='#d73027', label='Pushes toward churn'),\n",
    "    Patch(facecolor='#4575b4', label='Pushes toward retention'),\n",
    "]\n",
    "fig.legend(handles=legend_elements, loc='lower center', ncol=2, fontsize=9, bbox_to_anchor=(0.5, -0.04))\n",
    "fig.suptitle('SHAP Force Bars — Three Example Customers', fontsize=12, y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0015-4000-8000-000000000015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plain-English interpretation of the top 3 global SHAP drivers ---\n",
    "\n",
    "# Rank all features by mean absolute SHAP on the test set\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature':       FEATURE_COLUMNS,\n",
    "    'mean_abs_shap': np.abs(sv).mean(axis=0),\n",
    "}).sort_values('mean_abs_shap', ascending=False).reset_index(drop=True)\n",
    "\n",
    "top3 = feature_importance.head(3)\n",
    "\n",
    "print('Top 3 global SHAP drivers (test set):')\n",
    "print('=' * 60)\n",
    "for rank, row in enumerate(top3.itertuples(), start=1):\n",
    "    print(f'#{rank}: {row.feature}  (mean |SHAP| = {row.mean_abs_shap:.4f})')\n",
    "\n",
    "print()\n",
    "print('Plain-English interpretation:')\n",
    "print('-' * 60)\n",
    "\n",
    "# Build dynamic interpretation based on actual top-3 feature names\n",
    "interpretations = {\n",
    "    'Contract_Month-to-month': (\n",
    "        'Being on a month-to-month contract is the single strongest churn signal. '\n",
    "        'Customers with no long-term commitment show substantially higher predicted churn probability. '\n",
    "        'Offering discounted annual contracts to month-to-month customers is the highest-leverage retention action.'\n",
    "    ),\n",
    "    'is_month_to_month': (\n",
    "        'Being on a month-to-month contract is the single strongest churn signal. '\n",
    "        'Customers with no long-term commitment show substantially higher predicted churn probability. '\n",
    "        'Offering discounted annual contracts to month-to-month customers is the highest-leverage retention action.'\n",
    "    ),\n",
    "    'tenure': (\n",
    "        'Tenure is the most powerful retention signal: customers who have been subscribers for longer '\n",
    "        'are much less likely to churn. Short-tenure customers (< 12 months) are in the highest-risk window '\n",
    "        'and benefit most from early intervention (onboarding calls, first-year loyalty perks).'\n",
    "    ),\n",
    "    'InternetService_Fiber optic': (\n",
    "        'Fiber optic internet subscribers churn at a higher rate than DSL or no-internet customers. '\n",
    "        'This may reflect unmet service quality expectations (speed, reliability) for a premium product. '\n",
    "        'Proactive technical check-ins for fiber customers in their first 6 months could reduce churn.'\n",
    "    ),\n",
    "    'is_fiber_optic': (\n",
    "        'Fiber optic internet subscribers churn at a higher rate than DSL or no-internet customers. '\n",
    "        'This may reflect unmet service quality expectations (speed, reliability) for a premium product. '\n",
    "        'Proactive technical check-ins for fiber customers in their first 6 months could reduce churn.'\n",
    "    ),\n",
    "    'MonthlyCharges': (\n",
    "        'Higher monthly charges increase churn risk. Customers paying above the median ($58.65) who are '\n",
    "        'also on month-to-month contracts represent a compounded risk. '\n",
    "        'Value-affirmation messaging (highlighting features they are paying for but not using) may help.'\n",
    "    ),\n",
    "    'TotalCharges': (\n",
    "        'Total charges is a proxy for long-term value delivered. Low total charges relative to tenure '\n",
    "        '(captured also via total_charges_gap) can signal underutilisation, increasing churn risk. '\n",
    "        'Customers with low total charges despite moderate tenure may benefit from service upgrade offers.'\n",
    "    ),\n",
    "    'total_charges_gap': (\n",
    "        'The gap between expected cumulative charges and actual total charges flags anomalies — discounts, '\n",
    "        'plan downgrades, or data corrections. A positive gap (paid less than expected) may reflect '\n",
    "        'promotional customers whose loyalty is rate-sensitive, raising churn risk post-promotion.'\n",
    "    ),\n",
    "    'PaymentMethod_Electronic check': (\n",
    "        'Electronic check payers show elevated churn compared to automatic payment methods. '\n",
    "        'This may reflect lower financial commitment or friction. Migrating customers to autopay '\n",
    "        '(credit card or bank transfer) via a small incentive is an easy conversion with churn benefit.'\n",
    "    ),\n",
    "    'is_electronic_check': (\n",
    "        'Electronic check payers show elevated churn compared to automatic payment methods. '\n",
    "        'This may reflect lower financial commitment or friction. Migrating customers to autopay '\n",
    "        '(credit card or bank transfer) via a small incentive is an easy conversion with churn benefit.'\n",
    "    ),\n",
    "    'monthly_per_tenure': (\n",
    "        'The ratio of monthly charges to tenure captures how much a customer pays per month relative to '\n",
    "        'their relationship length. High ratios (new customers on expensive plans) are a churn risk. '\n",
    "        'These customers have not yet received enough value to justify the cost in their own assessment.'\n",
    "    ),\n",
    "    'OnlineSecurity': (\n",
    "        'Customers without online security add-ons churn more. Security services increase product depth '\n",
    "        'and switching cost. Offering a free 3-month trial of OnlineSecurity to at-risk customers '\n",
    "        'could meaningfully reduce churn in the short term.'\n",
    "    ),\n",
    "    'TechSupport': (\n",
    "        'Lack of tech support subscription correlates with higher churn, possibly because unresolved '\n",
    "        'technical problems drive dissatisfaction. Proactive outreach offering discounted TechSupport '\n",
    "        'to customers with recent trouble tickets is a targeted retention play.'\n",
    "    ),\n",
    "    'num_services': (\n",
    "        'Customers subscribing to more bundled services churn less — each additional service increases '\n",
    "        'switching cost and perceived value. Customers with only 1–2 services are the highest retention '\n",
    "        'opportunity; upsell campaigns targeting depth of product adoption reduce churn probability.'\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Generic fallback for any feature not covered above\n",
    "generic = (\n",
    "    'This feature contributes meaningfully to the model. '\n",
    "    'Examine the beeswarm plot above to understand directional impact '\n",
    "    '(positive SHAP = increases churn risk, negative SHAP = decreases churn risk).'\n",
    ")\n",
    "\n",
    "for rank, row in enumerate(top3.itertuples(), start=1):\n",
    "    explanation = interpretations.get(row.feature, generic)\n",
    "    print(f'#{rank}: {row.feature}')\n",
    "    print(f'   {explanation}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0016-4000-8000-000000000016",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3 — Business Simulation\n",
    "\n",
    "**Intent:** Translate model predictions into business language. Key questions:\n",
    "- If we contact only the top 20% highest-scored customers, what fraction of actual churners do we reach?\n",
    "- What does the gain curve look like — how efficiently does score rank order predict churn?\n",
    "- At a $20 contact cost and $200 average revenue per saved churner, what is the break-even contact rate?\n",
    "\n",
    "**Assumptions (adjust for your actuals):**\n",
    "- Average cost of a retention contact (call, offer, etc.): **$20**\n",
    "- Average monthly revenue per saved churner (1-month equivalent): **$200**\n",
    "- Intervention effectiveness (fraction of contacted churners who are successfully retained): **40%**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0017-4000-8000-000000000017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business parameters\n",
    "COST_PER_CONTACT   = 20    # USD — cost of one retention outreach\n",
    "REVENUE_PER_SAVE   = 200   # USD — one month's revenue for a saved churner\n",
    "RETENTION_RATE     = 0.40  # fraction of contacted churners actually retained (lift ≠ 100%)\n",
    "\n",
    "n_test       = len(y_test)\n",
    "n_churners   = int(y_test.sum())\n",
    "\n",
    "print(f'Test set customers: {n_test:,}')\n",
    "print(f'Actual churners:    {n_churners:,}  ({n_churners/n_test:.1%} churn rate)')\n",
    "print()\n",
    "\n",
    "# --- Gain curve ---\n",
    "# Sort customers descending by predicted score, then compute the cumulative\n",
    "# fraction of churners captured as we contact increasing fractions of the population.\n",
    "\n",
    "sort_order   = np.argsort(cal_proba)[::-1]     # high score first\n",
    "y_sorted     = y_test.values[sort_order]\n",
    "cum_churners = np.cumsum(y_sorted)\n",
    "pct_customers = np.arange(1, n_test + 1) / n_test\n",
    "pct_churners_captured = cum_churners / n_churners\n",
    "\n",
    "# What % of churners do we reach if we contact the top 20%?\n",
    "top20_cutoff = int(n_test * 0.20)\n",
    "pct_captured_top20 = cum_churners[top20_cutoff - 1] / n_churners\n",
    "print(f'Contacting top 20%: captures {pct_captured_top20:.1%} of all actual churners')\n",
    "print(f'  (random baseline would capture 20% — model lift = {pct_captured_top20/0.20:.2f}x)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0018-4000-8000-000000000018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Gain curve plot ---\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "ax.plot(pct_customers * 100, pct_churners_captured * 100,\n",
    "        color='steelblue', linewidth=2, label='Model gain curve')\n",
    "ax.plot([0, 100], [0, 100], '--', color='gray', linewidth=1, label='Random baseline')\n",
    "ax.plot([0, 100], [0, 100 * n_churners / n_test, 100, 100][:2],\n",
    "        ':', color='green', linewidth=1, alpha=0.7)\n",
    "\n",
    "# Highlight the top-20% operating point\n",
    "ax.axvline(20, color='darkorange', linestyle='--', linewidth=1.2, alpha=0.8)\n",
    "ax.axhline(pct_captured_top20 * 100, color='darkorange', linestyle='--', linewidth=1.2, alpha=0.8)\n",
    "ax.scatter([20], [pct_captured_top20 * 100], color='darkorange', s=80, zorder=5)\n",
    "ax.annotate(\n",
    "    f'Top 20% contacts\\ncaptures {pct_captured_top20:.1%} of churners',\n",
    "    xy=(20, pct_captured_top20 * 100),\n",
    "    xytext=(30, pct_captured_top20 * 100 - 10),\n",
    "    fontsize=9,\n",
    "    arrowprops=dict(arrowstyle='->', color='darkorange'),\n",
    "    color='darkorange',\n",
    ")\n",
    "\n",
    "ax.set_xlabel('% of customers contacted (sorted by score, highest first)', fontsize=10)\n",
    "ax.set_ylabel('% of actual churners captured', fontsize=10)\n",
    "ax.set_title('Gain Curve — Churn Propensity Model 3', fontsize=11)\n",
    "ax.set_xlim(0, 100)\n",
    "ax.set_ylim(0, 105)\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0019-4000-8000-000000000019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cost-benefit analysis at each contact rate ---\n",
    "contact_rates = np.linspace(0.01, 1.0, 100)\n",
    "costs, revenues, net_values = [], [], []\n",
    "\n",
    "for rate in contact_rates:\n",
    "    n_contacted       = int(n_test * rate)\n",
    "    n_churners_caught = int(cum_churners[n_contacted - 1])\n",
    "\n",
    "    total_cost    = n_contacted * COST_PER_CONTACT\n",
    "    total_revenue = n_churners_caught * RETENTION_RATE * REVENUE_PER_SAVE\n",
    "    net           = total_revenue - total_cost\n",
    "\n",
    "    costs.append(total_cost)\n",
    "    revenues.append(total_revenue)\n",
    "    net_values.append(net)\n",
    "\n",
    "costs      = np.array(costs)\n",
    "revenues   = np.array(revenues)\n",
    "net_values = np.array(net_values)\n",
    "\n",
    "# Break-even: first contact rate where revenue >= cost\n",
    "breakeven_idx  = np.argmax(np.array(net_values) >= 0) if np.any(np.array(net_values) >= 0) else None\n",
    "optimal_idx    = np.argmax(net_values)   # contact rate that maximises net value\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "# Left: cost vs revenue\n",
    "ax = axes[0]\n",
    "ax.plot(contact_rates * 100, costs / 1000,    color='tomato',    linewidth=2, label=f'Cost ($20 × contacts)')\n",
    "ax.plot(contact_rates * 100, revenues / 1000, color='seagreen',  linewidth=2, label=f'Revenue saved ($200 × {RETENTION_RATE:.0%} × churners caught)')\n",
    "\n",
    "if breakeven_idx is not None:\n",
    "    bx = contact_rates[breakeven_idx] * 100\n",
    "    by = costs[breakeven_idx] / 1000\n",
    "    ax.axvline(bx, color='navy', linestyle='--', linewidth=1, alpha=0.7)\n",
    "    ax.annotate(f'Break-even\\n~{bx:.0f}% contacted',\n",
    "                xy=(bx, by), xytext=(bx + 5, by + 2),\n",
    "                fontsize=8, color='navy',\n",
    "                arrowprops=dict(arrowstyle='->', color='navy'))\n",
    "\n",
    "ax.set_xlabel('% of customers contacted', fontsize=10)\n",
    "ax.set_ylabel('USD ($000s)', fontsize=10)\n",
    "ax.set_title('Intervention Cost vs. Revenue Saved', fontsize=11)\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# Right: net value curve\n",
    "ax2 = axes[1]\n",
    "ax2.plot(contact_rates * 100, net_values / 1000, color='steelblue', linewidth=2)\n",
    "ax2.axhline(0, color='gray', linewidth=0.8, linestyle='--')\n",
    "ax2.fill_between(contact_rates * 100, net_values / 1000, 0,\n",
    "                 where=net_values >= 0, alpha=0.15, color='seagreen', label='Net positive')\n",
    "ax2.fill_between(contact_rates * 100, net_values / 1000, 0,\n",
    "                 where=net_values < 0, alpha=0.15, color='tomato',   label='Net negative')\n",
    "\n",
    "# Mark optimal point\n",
    "ox = contact_rates[optimal_idx] * 100\n",
    "oy = net_values[optimal_idx] / 1000\n",
    "ax2.scatter([ox], [oy], color='steelblue', s=80, zorder=5)\n",
    "ax2.annotate(f'Optimal: {ox:.0f}% contacted\\nNet value: ${oy:.1f}k',\n",
    "             xy=(ox, oy), xytext=(ox + 5, oy - 3),\n",
    "             fontsize=8, color='steelblue',\n",
    "             arrowprops=dict(arrowstyle='->', color='steelblue'))\n",
    "\n",
    "ax2.set_xlabel('% of customers contacted', fontsize=10)\n",
    "ax2.set_ylabel('Net value ($000s)', fontsize=10)\n",
    "ax2.set_title('Break-Even Analysis — Net Business Value', fontsize=11)\n",
    "ax2.legend(fontsize=8)\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'Assumptions: contact cost=${COST_PER_CONTACT}, revenue/save=${REVENUE_PER_SAVE}, retention rate={RETENTION_RATE:.0%}')\n",
    "if breakeven_idx is not None:\n",
    "    print(f'Break-even at: ~{contact_rates[breakeven_idx]*100:.0f}% of customers contacted')\n",
    "print(f'Optimal contact rate: {contact_rates[optimal_idx]*100:.0f}%  →  net value = ${net_values[optimal_idx]:,.0f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0020-4000-8000-000000000020",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4 — Slice Analysis\n",
    "\n",
    "**Intent:** Measure whether model performance is consistent across important customer subgroups.\n",
    "Large AUC disparities indicate the model may perform poorly for certain segments, which has\n",
    "both fairness and business implications. Segments with low AUC may require sub-models or\n",
    "additional features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0021-4000-8000-000000000021",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_auc(df, segment_col, segment_values=None):\n",
    "    \"\"\"\n",
    "    Compute per-segment AUC, count, actual churn rate, and mean predicted score.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df              : test DataFrame with 'Churn', 'pred_proba', and segment_col\n",
    "    segment_col     : column name identifying the segment\n",
    "    segment_values  : optional list of segment values to include (e.g. for one-hot encoded cols)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame sorted by AUC descending\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    if segment_values is None:\n",
    "        segment_values = sorted(df[segment_col].unique())\n",
    "\n",
    "    for val in segment_values:\n",
    "        mask = df[segment_col] == val\n",
    "        subset = df[mask]\n",
    "        n      = len(subset)\n",
    "        if n < 20:\n",
    "            continue\n",
    "        churn_rate = subset['Churn'].mean()\n",
    "        mean_score = subset['pred_proba'].mean()\n",
    "\n",
    "        if subset['Churn'].nunique() < 2:\n",
    "            # Cannot compute AUC with only one class\n",
    "            auc = np.nan\n",
    "        else:\n",
    "            auc = roc_auc_score(subset['Churn'], subset['pred_proba'])\n",
    "\n",
    "        rows.append({\n",
    "            'segment':              f'{segment_col} = {val}',\n",
    "            'count':                n,\n",
    "            'churn_rate_actual':    churn_rate,\n",
    "            'mean_predicted_score': mean_score,\n",
    "            'auc':                  auc,\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows).sort_values('auc', ascending=False).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Contract type — derive from one-hot encoded columns\n",
    "df_test['contract_type'] = np.select(\n",
    "    [\n",
    "        df_test['Contract_Month-to-month'] == 1,\n",
    "        df_test['Contract_One year']        == 1,\n",
    "        df_test['Contract_Two year']        == 1,\n",
    "    ],\n",
    "    ['Month-to-month', 'One year', 'Two year'],\n",
    "    default='Unknown'\n",
    ")\n",
    "\n",
    "# InternetService type\n",
    "df_test['internet_service'] = np.select(\n",
    "    [\n",
    "        df_test['InternetService_DSL']         == 1,\n",
    "        df_test['InternetService_Fiber optic'] == 1,\n",
    "        df_test['InternetService_No']          == 1,\n",
    "    ],\n",
    "    ['DSL', 'Fiber optic', 'No internet'],\n",
    "    default='Unknown'\n",
    ")\n",
    "\n",
    "# Run slice AUC for each grouping dimension\n",
    "slices_contract = slice_auc(df_test, 'contract_type',    ['Month-to-month', 'One year', 'Two year'])\n",
    "slices_internet = slice_auc(df_test, 'internet_service', ['DSL', 'Fiber optic', 'No internet'])\n",
    "slices_senior   = slice_auc(df_test, 'SeniorCitizen',    [0, 1])\n",
    "\n",
    "# Rename SeniorCitizen values for readability\n",
    "slices_senior['segment'] = slices_senior['segment'].str.replace(\n",
    "    'SeniorCitizen = 0', 'SeniorCitizen = No'\n",
    ").str.replace('SeniorCitizen = 1', 'SeniorCitizen = Yes')\n",
    "\n",
    "slice_summary = pd.concat([slices_contract, slices_internet, slices_senior], ignore_index=True)\n",
    "\n",
    "print('Slice AUC Summary')\n",
    "print('=' * 80)\n",
    "print(slice_summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0022-4000-8000-000000000022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Slice AUC bar chart ---\n",
    "fig, ax = plt.subplots(figsize=(9, 5))\n",
    "\n",
    "segments  = slice_summary['segment'].tolist()\n",
    "auc_vals  = slice_summary['auc'].tolist()\n",
    "counts    = slice_summary['count'].tolist()\n",
    "\n",
    "# Color-code bars: good AUC (>0.75) green, moderate (0.65–0.75) orange, lower red\n",
    "bar_colors = [\n",
    "    'seagreen' if v >= 0.75 else ('darkorange' if v >= 0.65 else 'tomato')\n",
    "    for v in auc_vals\n",
    "]\n",
    "\n",
    "bars = ax.barh(segments, auc_vals, color=bar_colors, edgecolor='white', height=0.6)\n",
    "\n",
    "# Annotate with AUC value and count\n",
    "for bar, auc_val, n in zip(bars, auc_vals, counts):\n",
    "    if not np.isnan(auc_val):\n",
    "        ax.text(\n",
    "            bar.get_width() + 0.005, bar.get_y() + bar.get_height() / 2,\n",
    "            f'{auc_val:.3f}  (n={n:,})',\n",
    "            va='center', ha='left', fontsize=9,\n",
    "        )\n",
    "\n",
    "# Overall AUC reference line\n",
    "overall_auc = roc_auc_score(y_test, cal_proba)\n",
    "ax.axvline(overall_auc, color='navy', linestyle='--', linewidth=1.3, label=f'Overall AUC = {overall_auc:.3f}')\n",
    "\n",
    "ax.set_xlim(0.4, 1.05)\n",
    "ax.set_xlabel('AUC (test set)', fontsize=10)\n",
    "ax.set_title('Slice Analysis — AUC by Segment', fontsize=11)\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0023-4000-8000-000000000023",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5 — Failure Mode Analysis\n",
    "\n",
    "**Intent:** Characterise the model's systematic errors. Understanding *who* the model misses\n",
    "informs whether additional features, data sources, or a separate sub-model would help.\n",
    "\n",
    "- **False Negatives (FN):** customers who actually churned but whom the model scored LOW (< 0.3).\n",
    "  These are the most operationally dangerous errors — we will send no intervention.\n",
    "- **False Positives (FP):** customers flagged HIGH (> 0.7) who did not churn.\n",
    "  These waste retention budget and may annoy loyal customers with unsolicited offers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0024-4000-8000-000000000024",
   "metadata": {},
   "outputs": [],
   "source": [
    "FN_SCORE_CEILING = 0.30   # churners scored below this are false negatives\n",
    "FP_SCORE_FLOOR   = 0.70   # non-churners scored above this are false positives\n",
    "\n",
    "mask_fn  = (df_test['Churn'] == 1) & (df_test['pred_proba'] < FN_SCORE_CEILING)\n",
    "mask_fp  = (df_test['Churn'] == 0) & (df_test['pred_proba'] > FP_SCORE_FLOOR)\n",
    "mask_tn  = (df_test['Churn'] == 0) & (df_test['pred_proba'] < FN_SCORE_CEILING)   # baseline comparison\n",
    "\n",
    "df_fn = df_test[mask_fn].copy()\n",
    "df_fp = df_test[mask_fp].copy()\n",
    "df_tn = df_test[mask_tn].copy()   # true negatives: low-scored non-churners\n",
    "\n",
    "print(f'False Negatives (churned, scored < {FN_SCORE_CEILING}): {len(df_fn):,} customers')\n",
    "print(f'False Positives (retained, scored > {FP_SCORE_FLOOR}): {len(df_fp):,} customers')\n",
    "print(f'True Negatives  (retained, scored < {FN_SCORE_CEILING}): {len(df_tn):,} customers  (comparison group)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0025-4000-8000-000000000025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_segment(df_group, df_overall, label):\n",
    "    \"\"\"\n",
    "    Compute a profiling table comparing a failure group to the overall test set.\n",
    "    Shows the distribution of key categorical and continuous features.\n",
    "    \"\"\"\n",
    "    profile = pd.DataFrame()\n",
    "    profile.index.name = 'feature'\n",
    "\n",
    "    def pct(series):\n",
    "        return series.mean() * 100\n",
    "\n",
    "    rows = {}\n",
    "\n",
    "    # Categorical distributions\n",
    "    rows['Contract: Month-to-month (%)']   = (pct(df_group['Contract_Month-to-month']),    pct(df_overall['Contract_Month-to-month']))\n",
    "    rows['Contract: One year (%)']         = (pct(df_group['Contract_One year']),           pct(df_overall['Contract_One year']))\n",
    "    rows['Contract: Two year (%)']         = (pct(df_group['Contract_Two year']),           pct(df_overall['Contract_Two year']))\n",
    "    rows['InternetService: Fiber optic (%)'] = (pct(df_group['InternetService_Fiber optic']), pct(df_overall['InternetService_Fiber optic']))\n",
    "    rows['InternetService: DSL (%)']       = (pct(df_group['InternetService_DSL']),         pct(df_overall['InternetService_DSL']))\n",
    "    rows['InternetService: None (%)']      = (pct(df_group['InternetService_No']),          pct(df_overall['InternetService_No']))\n",
    "    rows['SeniorCitizen (%)']              = (pct(df_group['SeniorCitizen']),               pct(df_overall['SeniorCitizen']))\n",
    "    rows['Electronic check payer (%)']     = (pct(df_group['is_electronic_check']),         pct(df_overall['is_electronic_check']))\n",
    "\n",
    "    # Continuous features\n",
    "    rows['Median tenure (months)']         = (df_group['tenure'].median(),             df_overall['tenure'].median())\n",
    "    rows['Median MonthlyCharges ($)']      = (df_group['MonthlyCharges'].median(),     df_overall['MonthlyCharges'].median())\n",
    "    rows['Mean num_services']              = (df_group['num_services'].mean(),         df_overall['num_services'].mean())\n",
    "    rows['Pred score (mean)']              = (df_group['pred_proba'].mean(),           df_overall['pred_proba'].mean())\n",
    "\n",
    "    result = pd.DataFrame.from_dict(rows, orient='index', columns=[label, 'Overall test set'])\n",
    "    result.index.name = 'Feature'\n",
    "    return result\n",
    "\n",
    "\n",
    "fn_profile = profile_segment(df_fn, df_test, f'False Negatives (n={len(df_fn)})')\n",
    "fp_profile = profile_segment(df_fp, df_test, f'False Positives (n={len(df_fp)})')\n",
    "\n",
    "print('--- False Negatives Profile (churned but scored LOW) ---')\n",
    "print(fn_profile.round(2).to_string())\n",
    "print()\n",
    "print('--- False Positives Profile (retained but scored HIGH) ---')\n",
    "print(fp_profile.round(2).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0026-4000-8000-000000000026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visual profile comparison for False Negatives and False Positives ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "profile_features = [\n",
    "    ('Contract\\nMonth-to-month', 'Contract_Month-to-month'),\n",
    "    ('Contract\\nOne year',       'Contract_One year'),\n",
    "    ('Contract\\nTwo year',       'Contract_Two year'),\n",
    "    ('Fiber\\nOptic',             'InternetService_Fiber optic'),\n",
    "    ('DSL',                      'InternetService_DSL'),\n",
    "    ('No internet',              'InternetService_No'),\n",
    "    ('Senior\\nCitizen',          'SeniorCitizen'),\n",
    "    ('Electronic\\nCheck',        'is_electronic_check'),\n",
    "]\n",
    "\n",
    "labels = [f[0] for f in profile_features]\n",
    "cols   = [f[1] for f in profile_features]\n",
    "x      = np.arange(len(labels))\n",
    "w      = 0.35\n",
    "\n",
    "# False Negatives panel\n",
    "ax = axes[0]\n",
    "fn_rates   = [df_fn[c].mean() * 100 if len(df_fn) > 0 else 0 for c in cols]\n",
    "all_rates  = [df_test[c].mean() * 100 for c in cols]\n",
    "bars1 = ax.bar(x - w/2, fn_rates,  width=w, color='tomato',    label=f'False Negatives (n={len(df_fn)})', edgecolor='white')\n",
    "bars2 = ax.bar(x + w/2, all_rates, width=w, color='steelblue', label='Overall test set',                  edgecolor='white', alpha=0.7)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels, fontsize=8)\n",
    "ax.set_ylabel('% of customers', fontsize=9)\n",
    "ax.set_title(f'False Negatives Profile\\n(Churned, scored < {FN_SCORE_CEILING})', fontsize=10)\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "ax.set_ylim(0, 100)\n",
    "\n",
    "# False Positives panel\n",
    "ax2 = axes[1]\n",
    "fp_rates   = [df_fp[c].mean() * 100 if len(df_fp) > 0 else 0 for c in cols]\n",
    "bars3 = ax2.bar(x - w/2, fp_rates,  width=w, color='darkorange', label=f'False Positives (n={len(df_fp)})', edgecolor='white')\n",
    "bars4 = ax2.bar(x + w/2, all_rates, width=w, color='steelblue',  label='Overall test set',                  edgecolor='white', alpha=0.7)\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(labels, fontsize=8)\n",
    "ax2.set_ylabel('% of customers', fontsize=9)\n",
    "ax2.set_title(f'False Positives Profile\\n(Retained, scored > {FP_SCORE_FLOOR})', fontsize=10)\n",
    "ax2.legend(fontsize=8)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "ax2.set_ylim(0, 100)\n",
    "\n",
    "fig.suptitle('Failure Mode Profiles vs. Overall Test Set', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0027-4000-8000-000000000027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Tenure distribution for failure modes ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "bins = np.arange(0, 75, 6)   # 0–72 months in 6-month buckets\n",
    "\n",
    "ax = axes[0]\n",
    "if len(df_fn) > 0:\n",
    "    ax.hist(df_fn['tenure'],   bins=bins, color='tomato',    alpha=0.75, label=f'False Negatives (n={len(df_fn)})', density=True)\n",
    "ax.hist(df_test[df_test['Churn']==1]['tenure'], bins=bins, color='steelblue', alpha=0.5,\n",
    "        label='All actual churners', density=True)\n",
    "ax.set_xlabel('Tenure (months)', fontsize=9)\n",
    "ax.set_ylabel('Density', fontsize=9)\n",
    "ax.set_title('Tenure Distribution\\nFalse Negatives vs. All Churners', fontsize=10)\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "ax2 = axes[1]\n",
    "if len(df_fp) > 0:\n",
    "    ax2.hist(df_fp['tenure'],   bins=bins, color='darkorange', alpha=0.75, label=f'False Positives (n={len(df_fp)})', density=True)\n",
    "ax2.hist(df_test[df_test['Churn']==0]['tenure'], bins=bins, color='steelblue', alpha=0.5,\n",
    "         label='All actual retained', density=True)\n",
    "ax2.set_xlabel('Tenure (months)', fontsize=9)\n",
    "ax2.set_ylabel('Density', fontsize=9)\n",
    "ax2.set_title('Tenure Distribution\\nFalse Positives vs. All Retained', fontsize=10)\n",
    "ax2.legend(fontsize=8)\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print tenure comparison statistics\n",
    "print('Tenure summary by error type:')\n",
    "print(f'  All churners (test):          median={df_test[df_test[\"Churn\"]==1][\"tenure\"].median():.0f}m  mean={df_test[df_test[\"Churn\"]==1][\"tenure\"].mean():.1f}m')\n",
    "if len(df_fn) > 0:\n",
    "    print(f'  False Negatives:              median={df_fn[\"tenure\"].median():.0f}m  mean={df_fn[\"tenure\"].mean():.1f}m')\n",
    "print(f'  All retained (test):          median={df_test[df_test[\"Churn\"]==0][\"tenure\"].median():.0f}m  mean={df_test[df_test[\"Churn\"]==0][\"tenure\"].mean():.1f}m')\n",
    "if len(df_fp) > 0:\n",
    "    print(f'  False Positives:              median={df_fp[\"tenure\"].median():.0f}m  mean={df_fp[\"tenure\"].mean():.1f}m')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0028-4000-8000-000000000028",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6 — Score Distribution\n",
    "\n",
    "**Intent:** Visualise the separation between predicted scores for churners vs. retained customers.\n",
    "Good separation means the model assigns meaningfully different probabilities to each class.\n",
    "The KS statistic quantifies this separation; higher is better (max = 1.0).\n",
    "\n",
    "The Population Stability Index (PSI) concept is introduced here for future monitoring:\n",
    "once the model is in production, comparing the live score distribution against this baseline\n",
    "will detect when the customer population has shifted, triggering a retraining alert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0029-4000-8000-000000000029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate scores by actual class\n",
    "scores_churn    = cal_proba[y_test.values == 1]   # scores for actual churners\n",
    "scores_retained = cal_proba[y_test.values == 0]   # scores for retained customers\n",
    "\n",
    "# KS statistic: maximum absolute difference between the two CDFs\n",
    "ks_stat, ks_pval = ks_2samp(scores_churn, scores_retained)\n",
    "\n",
    "print(f'KS statistic: {ks_stat:.4f}  (p-value: {ks_pval:.2e})')\n",
    "print(f'  Interpretation: the two score distributions differ by at most {ks_stat:.1%} at any threshold.')\n",
    "print(f'  A KS > 0.40 is generally considered strong separation for binary classifiers.')\n",
    "print()\n",
    "print(f'Score distribution statistics:')\n",
    "print(f'  Churners    — mean: {scores_churn.mean():.4f}  median: {np.median(scores_churn):.4f}  std: {scores_churn.std():.4f}')\n",
    "print(f'  Retained    — mean: {scores_retained.mean():.4f}  median: {np.median(scores_retained):.4f}  std: {scores_retained.std():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0030-4000-8000-000000000030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Histogram of predicted probabilities, split by actual class ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "bins = np.linspace(0, 1, 41)   # 40 bins across [0, 1]\n",
    "\n",
    "# Left: overlapping histograms (density)\n",
    "ax = axes[0]\n",
    "ax.hist(scores_retained, bins=bins, density=True, alpha=0.65, color='steelblue',  label=f'Retained (n={len(scores_retained):,})',  edgecolor='white')\n",
    "ax.hist(scores_churn,    bins=bins, density=True, alpha=0.65, color='tomato',     label=f'Churned  (n={len(scores_churn):,})',     edgecolor='white')\n",
    "\n",
    "# Vertical lines at class means\n",
    "ax.axvline(scores_retained.mean(), color='steelblue', linestyle='--', linewidth=1.5, label=f'Retained mean = {scores_retained.mean():.3f}')\n",
    "ax.axvline(scores_churn.mean(),    color='tomato',    linestyle='--', linewidth=1.5, label=f'Churned mean  = {scores_churn.mean():.3f}')\n",
    "\n",
    "# KS annotation\n",
    "ax.text(0.02, 0.95, f'KS = {ks_stat:.3f}', transform=ax.transAxes,\n",
    "        fontsize=10, va='top', color='black',\n",
    "        bbox=dict(boxstyle='round,pad=0.3', facecolor='lightyellow', edgecolor='gray', alpha=0.8))\n",
    "\n",
    "ax.set_xlabel('Predicted churn probability', fontsize=10)\n",
    "ax.set_ylabel('Density', fontsize=10)\n",
    "ax.set_title('Score Distribution by Actual Class', fontsize=11)\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# Right: CDF curves (to illustrate KS gap visually)\n",
    "ax2 = axes[1]\n",
    "x_vals = np.linspace(0, 1, 300)\n",
    "\n",
    "def empirical_cdf(scores, x_vals):\n",
    "    return np.array([(scores <= x).mean() for x in x_vals])\n",
    "\n",
    "cdf_churn    = empirical_cdf(scores_churn,    x_vals)\n",
    "cdf_retained = empirical_cdf(scores_retained, x_vals)\n",
    "\n",
    "ax2.plot(x_vals, cdf_retained, color='steelblue', linewidth=2, label='Retained (CDF)')\n",
    "ax2.plot(x_vals, cdf_churn,    color='tomato',    linewidth=2, label='Churned (CDF)')\n",
    "\n",
    "# Shade the KS gap at the point of maximum separation\n",
    "ks_x_idx = np.argmax(np.abs(cdf_retained - cdf_churn))\n",
    "ks_x     = x_vals[ks_x_idx]\n",
    "ks_y_lo  = min(cdf_retained[ks_x_idx], cdf_churn[ks_x_idx])\n",
    "ks_y_hi  = max(cdf_retained[ks_x_idx], cdf_churn[ks_x_idx])\n",
    "ax2.plot([ks_x, ks_x], [ks_y_lo, ks_y_hi], color='black', linewidth=2, linestyle='-', label=f'KS gap = {ks_stat:.3f}')\n",
    "ax2.annotate(\n",
    "    f'KS = {ks_stat:.3f}\\n@ score = {ks_x:.2f}',\n",
    "    xy=(ks_x, (ks_y_lo + ks_y_hi) / 2),\n",
    "    xytext=(ks_x + 0.08, (ks_y_lo + ks_y_hi) / 2),\n",
    "    fontsize=8,\n",
    "    arrowprops=dict(arrowstyle='->', color='black'),\n",
    ")\n",
    "\n",
    "ax2.set_xlabel('Predicted churn probability', fontsize=10)\n",
    "ax2.set_ylabel('Cumulative proportion', fontsize=10)\n",
    "ax2.set_title('Empirical CDFs — KS Separation', fontsize=11)\n",
    "ax2.legend(fontsize=8)\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0031-4000-8000-000000000031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Population Stability Index (PSI) concept and baseline computation ---\n",
    "\n",
    "print('Population Stability Index (PSI) — Monitoring Baseline')\n",
    "print('=' * 65)\n",
    "print()\n",
    "print('PSI measures how much a score distribution has shifted between two periods.')\n",
    "print('Use this baseline (test set) to compare against live production scores')\n",
    "print('in future monitoring runs to detect population drift.')\n",
    "print()\n",
    "print('PSI interpretation thresholds:')\n",
    "print('  PSI < 0.10  →  No significant change — model remains stable')\n",
    "print('  PSI 0.10–0.25 →  Moderate shift — investigate and monitor')\n",
    "print('  PSI > 0.25  →  Significant shift — consider retraining')\n",
    "print()\n",
    "\n",
    "# Compute PSI reference distribution (baseline = test set)\n",
    "# Decile-based bins (10 equal-frequency buckets)\n",
    "psi_bins     = np.percentile(cal_proba, np.linspace(0, 100, 11))  # 10 equal-frequency bins\n",
    "psi_bins[0]  = 0.0\n",
    "psi_bins[-1] = 1.0\n",
    "\n",
    "ref_counts = np.histogram(cal_proba, bins=psi_bins)[0]\n",
    "ref_dist   = ref_counts / ref_counts.sum()\n",
    "\n",
    "print('Baseline score distribution (test set, 10 decile buckets):')\n",
    "print(f'  Bucket boundaries: {np.round(psi_bins, 3).tolist()}')\n",
    "print(f'  Baseline proportions: {np.round(ref_dist, 4).tolist()}')\n",
    "print()\n",
    "print('To compute PSI against future production data:')\n",
    "print('''\n",
    "  def compute_psi(baseline_dist, current_scores, bins):\n",
    "      cur_counts = np.histogram(current_scores, bins=bins)[0]\n",
    "      cur_dist   = cur_counts / cur_counts.sum()\n",
    "      # Add epsilon to avoid log(0)\n",
    "      psi = np.sum(\n",
    "          (cur_dist - baseline_dist) * np.log((cur_dist + 1e-8) / (baseline_dist + 1e-8))\n",
    "      )\n",
    "      return psi\n",
    "''')\n",
    "\n",
    "# Save the PSI baseline so the monitoring pipeline can load it\n",
    "import json\n",
    "psi_baseline = {\n",
    "    'bin_edges':    list(np.round(psi_bins, 6)),\n",
    "    'bin_proportions': list(np.round(ref_dist, 6)),\n",
    "    'ks_stat':      round(float(ks_stat), 6),\n",
    "    'auc':          round(float(roc_auc_score(y_test, cal_proba)), 6),\n",
    "    'n_test':       int(len(y_test)),\n",
    "    'generated_by': '03_model_diagnostics.ipynb',\n",
    "}\n",
    "baseline_path = MODEL_DIR / 'psi_baseline.json'\n",
    "with open(baseline_path, 'w') as f:\n",
    "    json.dump(psi_baseline, f, indent=2)\n",
    "\n",
    "print(f'PSI baseline saved to: {baseline_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-0032-4000-8000-000000000032",
   "metadata": {},
   "source": [
    "---\n",
    "## Diagnostic Summary\n",
    "\n",
    "**Intent:** Consolidate all diagnostic metrics into a single printable summary for the model card."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4-0033-4000-8000-000000000033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all headline metrics computed across the notebook\n",
    "summary = {\n",
    "    # Data\n",
    "    'test_set_size':            int(len(y_test)),\n",
    "    'test_churn_rate':          round(float(y_test.mean()), 4),\n",
    "\n",
    "    # Section 1 — Thresholds\n",
    "    'auc':                      round(float(metrics_default['auc']), 4),\n",
    "    'default_threshold':        DEFAULT_THRESHOLD,\n",
    "    'default_precision':        round(float(metrics_default['precision']), 4),\n",
    "    'default_recall':           round(float(metrics_default['recall']), 4),\n",
    "    'default_f1':               round(float(metrics_default['f1']), 4),\n",
    "    'cost_threshold':           COST_THRESHOLD,\n",
    "    'cost_precision':           round(float(metrics_cost['precision']), 4),\n",
    "    'cost_recall':              round(float(metrics_cost['recall']), 4),\n",
    "    'cost_f1':                  round(float(metrics_cost['f1']), 4),\n",
    "\n",
    "    # Section 3 — Business simulation\n",
    "    'top20pct_churners_captured': round(float(pct_captured_top20), 4),\n",
    "    'optimal_contact_rate':     round(float(contact_rates[optimal_idx]), 4),\n",
    "    'optimal_net_value_usd':    round(float(net_values[optimal_idx]), 2),\n",
    "\n",
    "    # Section 5 — Failure modes\n",
    "    'n_false_negatives':        int(len(df_fn)),\n",
    "    'n_false_positives':        int(len(df_fp)),\n",
    "\n",
    "    # Section 6 — Score distribution\n",
    "    'ks_statistic':             round(float(ks_stat), 4),\n",
    "    'mean_score_churners':      round(float(scores_churn.mean()), 4),\n",
    "    'mean_score_retained':      round(float(scores_retained.mean()), 4),\n",
    "}\n",
    "\n",
    "print('DIAGNOSTIC SUMMARY — Model 3: Churn Propensity (LightGBM)')\n",
    "print('=' * 60)\n",
    "for k, v in summary.items():\n",
    "    print(f'  {k:<35s}: {v}')\n",
    "print('=' * 60)\n",
    "\n",
    "# Persist summary alongside model artifacts\n",
    "diagnostics_path = MODEL_DIR / 'diagnostics_summary.json'\n",
    "with open(diagnostics_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "print(f'\\nDiagnostics summary saved to: {diagnostics_path}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
